{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comp 47350 - Assignment 2  \n",
    "Tania Lopes *(22202398)*  \n",
    "Emma Cassar Torreggiani *(18389156)*\n",
    "\n",
    "\n",
    "\n",
    "# **Introduction**\n",
    "<span style=\"color: red; font-style: italic\">Refer to '22202398_Data Quality Report' for a brief introduction to the machine learning problem we would like to solve in addition to some key terminology to be familiar with. </span>\n",
    "\n",
    "This notebook will be split into 5 main parts.   \n",
    "- [1.0 Data Understanding and Preparation](#10-data-understanding-and-preparation)\n",
    "- [2.0 Predictive Modeling - Linear Regression](#20-predictive-modeling-linear-regression)\n",
    "- [3.0 Predictive Modeling - Logical Regression](#30-predictive-modeling-logistic-regression)\n",
    "- [4.0 Predictive Modeling - Random Forest](#40-predictive-modeling-random-forest)\n",
    "- [5.0 Conclusions and Optimization](#50-conclusions--optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1.0 Data Understanding and Preparation** \n",
    "\n",
    "## 1.1 Prepare environment and load data\n",
    "First things first, we need to import the packages that will help us with our data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\2211979658.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtree\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexport_graphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgraphviz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgraphviz\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSource\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'graphviz'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import textwrap\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.tree import export_graphviz\n",
    "\n",
    "import graphviz\n",
    "from graphviz import Source\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from tabulate import tabulate\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define some functions that will help us throughout to analyze the data and regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculateNthMode(dataFrame, n):\n",
    "    describeData = dataFrame.describe(datetime_is_numeric = True).T\n",
    "\n",
    "    indices = describeData.index.values.tolist()\n",
    "\n",
    "    secondMode = []\n",
    "    secondModeFreq = []\n",
    "    \n",
    "    for column in dataFrame:\n",
    "        secondMode.append(dataFrame[column].value_counts().index.tolist()[n-1])\n",
    "        secondModeFreq.append(dataFrame[column].value_counts().tolist()[n-1])\n",
    "\n",
    "    second_mode = pd.DataFrame(secondMode, columns=[f'{n}th mode']).set_index([indices]).T\n",
    "    second_mode_freq = pd.DataFrame(secondModeFreq, columns=[f'{n}th mode freq']).set_index([indices]).T\n",
    "\n",
    "    return second_mode, second_mode_freq\n",
    "\n",
    "\n",
    "def applyToRowsAndAppend(dataFrame, expression, columnName):\n",
    "    tempDataFrame_result = dataFrame.apply(expression, axis=1).rename(columnName)\n",
    "    tempDataFrame = pd.concat([dataFrame, tempDataFrame_result], axis=1)\n",
    "    return tempDataFrame\n",
    "\n",
    "\n",
    "def getDescriptiveStatisticsForContinuousFeatures(dataFrame):\n",
    "    # Get descriptive statistics, mode, second mode, and cardinality\n",
    "    stats = dataFrame.describe(datetime_is_numeric = True)\n",
    "\n",
    "    n_missing = pd.DataFrame((dataFrame.isnull()).sum(), columns=['# missing']).T\n",
    "    p_missing = pd.DataFrame(((dataFrame.isnull()).sum() / dataFrame.shape[0] * 100), columns=['% missing']).T\n",
    "\n",
    "    first_mode, first_modeFreq = calculateNthMode(dataFrame, 1)\n",
    "    second_mode, second_modeFreq = calculateNthMode(dataFrame, 2)\n",
    "    cardinality = pd.DataFrame(dataFrame.nunique(), columns=['cardinality']).T\n",
    "\n",
    "    # Concatenate statistics into one descriptive statistics table\n",
    "    stats = pd.concat([stats, n_missing, p_missing, first_mode, first_modeFreq, second_mode, second_modeFreq, cardinality]).T\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def getDescriptiveStatisticsForCategoricalFeatures(dataFrame):\n",
    "    # Calculate the number of missing entries for each of the continuous feature\n",
    "    stats = dataFrame.describe()\n",
    "\n",
    "    n_missing = pd.DataFrame((dataFrame == 'Missing').sum(), columns=['# missing']).T\n",
    "    p_missing = pd.DataFrame(((dataFrame == 'Missing').sum() / dataFrame.shape[0] * 100), columns=['% missing']).T\n",
    "\n",
    "    # Calculate second mode\n",
    "    second_mode, second_mode_frequency = calculateNthMode(dataFrame, 2)\n",
    "\n",
    "    # Concatenate statistics into one descriptive statistics table\n",
    "    stats = pd.concat([stats, n_missing, p_missing, second_mode, second_mode_frequency]).T\n",
    "\n",
    "    return stats\n",
    "\n",
    "\n",
    "def getValidityColumn(dataFrame, expression):\n",
    "    validity_result = applyToRowsAndAppend(dataFrame, expression, 'Validity')\n",
    "    \n",
    "    # Count the number of columns where validity is not equal to True\n",
    "    invalid_entry = validity_result['Validity'].value_counts().index.tolist()\n",
    "\n",
    "    if len(invalid_entry) == 1 and invalid_entry[0]:\n",
    "        print(\"The data passes the test.\")\n",
    "    else:\n",
    "        count_invalid_entry = len(validity_result.loc[validity_result['Validity'] == False])\n",
    "        print(f\"The data does not pass the test. \\n{count_invalid_entry} invalid entries.\")\n",
    "\n",
    "def getCorrelations(data, columns, abs_corr=False):\n",
    "    correlation = data[columns].corr(numeric_only=False)\n",
    "    if (abs_corr):\n",
    "        correlation = abs(correlation)\n",
    "    return correlation\n",
    "\n",
    "def plotCorrelationHeatmap(data, columns, ax=None, abs_corr=False):\n",
    "    # Print the correlation between the continuous data types\n",
    "    # Specify numeric_only as False to allow us to include correlation with case_month\n",
    "    correlation = getCorrelations(data, columns, abs_corr)\n",
    "\n",
    "    # Generate a mask for the upper triangle\n",
    "    corr_mask = np.zeros_like(correlation, dtype=np.bool_)\n",
    "    corr_mask[np.triu_indices_from(corr_mask)] = True\n",
    "\n",
    "    # Generate a custom colormap - cyan and red\n",
    "    corr_cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Use the provided ax or create a new one\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    \n",
    "    # Draw the heatmap with the mask and correct aspect ratio\n",
    "    sns.heatmap(correlation, annot=True, mask=corr_mask, cmap=corr_cmap, vmax=1, vmin=-1,\n",
    "                square=True, xticklabels=True, yticklabels=True,\n",
    "                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "    \n",
    "    # Set the title of the plot\n",
    "    ax.set_title('Correlation Heatmap')\n",
    "    \n",
    "    # Show the plot if no ax was provided\n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "def plotCrosstabHeatmap(data, column1, column2, ax=None):\n",
    "    # Create a cross-tabulation of two categorical variables\n",
    "    crosstab = pd.crosstab(data[column1], data[column2], normalize=True)\n",
    "\n",
    "    # Generate a custom colormap - cyan and red\n",
    "    crosstab_cmap = sns.light_palette((10/360 ,75/100,50/100), input=\"hls\", as_cmap=True)\n",
    "    # crosstab_cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
    "\n",
    "    # Use the provided ax or create a new one\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(8, 4))\n",
    "    \n",
    "    # Create a heatmap of the cross-tabulation\n",
    "    sns.heatmap(crosstab, annot=False, cmap=crosstab_cmap,\n",
    "                square=True, xticklabels=True, yticklabels=True,\n",
    "                linewidths=.5, cbar_kws={\"shrink\": .5}, ax=ax)\n",
    "\n",
    "\n",
    "def createStackedHistograms(data, group, to_count, bin_count=10, ax=None, i=None, j=None, filterExp=None, labelheights=None, ylim = 1):\n",
    "    \n",
    "    # group the data by the specified column\n",
    "    data = data.dropna().copy()\n",
    "    if filterExp != None:\n",
    "        data = data.loc[data.apply(filterExp, axis=1)]\n",
    "    if data[to_count].dtype == 'datetime64[ns]':\n",
    "        data.loc[:, to_count] = (data[to_count].apply(lambda x: (x.year - data[to_count].min().year) * 12 + x.month))\n",
    "    \n",
    "    grouped_data = data.groupby(group)[to_count]\n",
    "\n",
    "    values = data[to_count]\n",
    "    bin_range = np.linspace(values.min(), values.max(), bin_count + 1)\n",
    "    bin_width = bin_range[1]-bin_range[0]\n",
    "\n",
    "    # Create a list of zeros to be the bottom value of each histogram bar\n",
    "    bottom = 0\n",
    "    bottom_value = [bottom for i in range(bin_count)]\n",
    "    # Create a list of ones to be the bottom value of each histogram bar\n",
    "    top = ylim\n",
    "    top_value = [top for i in range(bin_count)]\n",
    "    # Create a list of zeros to be a placeholder for number of bars too small to show text\n",
    "    too_small_count = [0 for i in range(bin_count)]\n",
    "\n",
    "    for name, group_data in grouped_data:\n",
    "        # Get counts for histogram\n",
    "        hist, _ = np.histogram(group_data, bins=bin_range)\n",
    "        pct = hist / len(group_data) if len(group_data) != 0 else 0\n",
    "\n",
    "        # Draw the histogram for the first group\n",
    "        ax.bar(bin_range[:-1], pct, label=name, align='edge', width=bin_width,\n",
    "               bottom=bottom_value, edgecolor='black', linewidth=0.5)\n",
    "        ax.set_ylim(top=1)\n",
    "        if labelheights == None:\n",
    "            labelheights = (len(grouped_data) * 0.025,\n",
    "                            len(grouped_data) * 0.025)\n",
    "\n",
    "        # Count the values that will be too small to display in the center of the bar\n",
    "        threshold = labelheights[0]\n",
    "        if not isinstance(pct, np.ndarray):\n",
    "            increment = 1 if 0 < pct < threshold else 0\n",
    "            too_small_count = [tsc + increment for tsc in too_small_count]\n",
    "            bottom_value = [bv + pct for bv in bottom_value]\n",
    "        else:\n",
    "            increment = list(map(lambda h: 1 if 0 < h < threshold else 0, pct))\n",
    "            too_small_count = [tsc + i for tsc, i in zip(too_small_count, increment)]\n",
    "            bottom_value += pct\n",
    "\n",
    "        def createlabel(val):\n",
    "            x = bin_range[i]+bin_width/2\n",
    "            y = bottom_value[i]-val/2 + (too_small_count[i])*labelheights[1]\n",
    "            text_name = textwrap.shorten(str(name).replace(\"/\", \" / \").replace(\"-\",\" - \"), width=13, placeholder=\"...\")\n",
    "            val_text = int(round(val, 2)*100)\n",
    "            ax.text(\n",
    "                x=x, y=y, fontsize=6,\n",
    "                s=f\"{text_name}:\\n {val_text}%\",\n",
    "                ha='center', va='center', zorder=4\n",
    "            )\n",
    "\n",
    "            rect = plt.Rectangle((x-bin_width*.9/2, y-(labelheights[1]*.8)/2),\n",
    "                             width=bin_width*0.9, height=labelheights[1]*.8,\n",
    "                             zorder=3, alpha=0.5, facecolor=\"white\",\n",
    "                             linewidth=0.5, edgecolor='black')\n",
    "            ax.add_patch(rect)\n",
    "\n",
    "\n",
    "        # Create text labels for each bar\n",
    "        if not isinstance(pct, np.ndarray):\n",
    "            val = pct\n",
    "            if val != 0:\n",
    "                createlabel(val)\n",
    "\n",
    "        else:\n",
    "            for i, val in enumerate(pct):\n",
    "                if val != 0:\n",
    "                    createlabel(val)\n",
    "        \n",
    "        ax.set_xticks(bin_range)\n",
    "        ax.grid(axis='y', alpha=0.5, color='lightgrey', linestyle='--')\n",
    "        ax.set_yticks(np.linspace(bottom, top, 11))\n",
    "\n",
    "\n",
    "        # Axis labels\n",
    "        ax.set_xlabel(str(to_count).title().replace(\"_\", \" \"), labelpad=20)\n",
    "        ax.set_ylabel(f'Frequency of {group.replace(\"_\", \" \").title()}', labelpad=20)\n",
    "\n",
    "        title = f'{to_count} by {group}'.replace(\"_\", \" \").title()\n",
    "        ax.set_title(title, fontsize=18, pad=20)\n",
    "        ax.legend()\n",
    "\n",
    "\n",
    "def getUniqueValueCounts(data):\n",
    "    x, y = data.columns\n",
    "    unique_values = data[y].unique().tolist()\n",
    "    new_data = pd.DataFrame()\n",
    "\n",
    "    for val in unique_values:\n",
    "        # Select only the rows that match the criteria for this row.\n",
    "        data_vc = data.loc[lambda row: row[y] == val][x].value_counts()\n",
    "        # Get the distribution of values in column_y for each unique_value from column_x\n",
    "        data_counts = list(map(lambda count: round(\n",
    "            count / sum(data_vc), 2), data_vc.tolist()))\n",
    "        data_values = data_vc.index.tolist()\n",
    "        # Generate a new row entry to be inserted into the table for this category\n",
    "        data_vc = pd.DataFrame(data_counts, data_values, columns=[val]).T\n",
    "\n",
    "        # Add the row to the table for this graph\n",
    "        new_data = pd.concat([new_data, data_vc])\n",
    "    return new_data\n",
    "\n",
    "def createStackedBarChart(data, column_x, column_y, axs=None, i=None, j=None):\n",
    "    stacked_bar = data[[column_x, column_y]]\n",
    "\n",
    "    stacked_bars = getUniqueValueCounts(stacked_bar)\n",
    "\n",
    "    # Create plots\n",
    "    if type(axs) == 'NoneType' and (i == None and j == None):\n",
    "        plot = stacked_bars.plot.bar(stacked=True, edgecolor='#111111')\n",
    "    elif type(axs) != 'NoneType' and (i == None and j == None):\n",
    "        plot = stacked_bars.plot.bar(\n",
    "            stacked=True, ax=axs,  edgecolor='#111111')\n",
    "    elif type(axs) != 'NoneType' and (i == None and j != None):\n",
    "        plot = stacked_bars.plot.bar(\n",
    "            stacked=True, ax=axs[j], edgecolor='#111111')\n",
    "    else:\n",
    "        plot = stacked_bars.plot.bar(\n",
    "            stacked=True, ax=axs[i][j], edgecolor='#111111')\n",
    "\n",
    "    # Label parameters\n",
    "    label_text = []\n",
    "    for label in plot.get_xticklabels():\n",
    "        lt = label.get_text().replace(\"/\", \" / \")\n",
    "        label_text.append(\"\\n\".join(textwrap.wrap(lt, width=20)))\n",
    "    plot.set_xticklabels(label_text)\n",
    "    plot.tick_params(labelsize=8, rotation=45)\n",
    "\n",
    "    # Bar labels\n",
    "    for cont in plot.containers:\n",
    "        plot.bar_label(cont, fontsize=8, label_type='center')\n",
    "\n",
    "    # Axis labels\n",
    "    plot.set_xlabel(str(column_y).title().replace(\"_\", \" \"))\n",
    "    plot.set_ylabel(\"Frequency\")\n",
    "\n",
    "    # Split title text\n",
    "    title = \"\\n\".join(textwrap.wrap(\n",
    "        (f\"Frequency of {column_x} by {column_y}\").upper().replace(\"_\", \" \"), 20))\n",
    "    plot.set_title(title, fontsize=24, pad=20, wrap=True)\n",
    "\n",
    "    # Legend and grid\n",
    "    plot.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "    plot.grid(visible=True, which='minor',\n",
    "              color='#999999', linestyle='-', alpha=0.2)\n",
    "    plot.grid(visible=True, which='major', color='#666666', linestyle='-')\n",
    "\n",
    "    # Subplot spacing\n",
    "    plt.subplots_adjust(left=0.1,  # The position of the left edge of the subplots, as a fraction of the figure width.\n",
    "                        # bottom=0.1, # The position of the bottom edge of the subplots, as a fraction of the figure height.\n",
    "                        # The position of the right edge of the subplots, as a fraction of the figure width.\n",
    "                        right=1.5,\n",
    "                        # top=0.9,    # The position of the top edge of the subplots, as a fraction of the figure height.\n",
    "                        # The width of the padding between subplots, as a fraction of the average Axes width.\n",
    "                        wspace=1.5,\n",
    "                        # The height of the padding between subplots, as a fraction of the average Axes height.\n",
    "                        hspace=0.75\n",
    "                        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 Merging the datasets\n",
    "\n",
    "The original data sets from the CDC have been cleaned as part of Task 1. Each student had a different body of work for the this task, and one was selected as a starting point for this work. The cleaning steps have been applied to two data sets, and both 'covid19-cdc-22202398_final' and 'covid19-cdc-18389156_final.csv' will need to be loaded and combined for the modeling to begin.\n",
    "\n",
    "The Data Quality Report from the first assignment can be found in the accompanying pdf. The Data Quality Plan is included below. \n",
    "| Feature Name | Data Quality Issue | Handling Strategy | \n",
    "|--------------|--------------------|-------------------|\n",
    "| case_month | None | Keep as Is |\n",
    "| res_state| Redundant | Drop Feature |\n",
    "| state_fips_code | Redundant | Drop Feature|\n",
    "| res_county | Redundant and Ambiguous | Drop Feature |\n",
    "| county_fips_code | Missing Data | Impute using state_fips_code values * 1000 to generate new county_fips_codes when missing|\n",
    "| age_group | Missing Data |Impute using k-nearest neighbor algorithm |\n",
    "| sex | Missing Data |Impute using k-nearest neighbor algorithm |\n",
    "| race | Missing Data |Impute using k-nearest neighbor algorithm |\n",
    "| ethnicity | Missing Data |Impute using k-nearest neighbor algorithm |\n",
    "| case_positive_specimen_interval | Invalid Data | Take the absolute value of the data to make all values positive |\n",
    "| case_positive_specimen_interval | Not Applicable Data | Distinguish between 'Missing' values when the number is not applicable, versus when the number was truly not present in the set.|\n",
    "| case_positive_specimen_interval | Missing Data | Impute using k-nearest neighbor algorithm |\n",
    "| case_positive_specimen_interval | Outliers | Keep as is for now, after cleaning the data, re-evaluate|\n",
    "| case_onset_interval | Invalid Data | Take the absolute value of the data to make all values positive |\n",
    "| case_onset_interval | Not Applicable Data | Distinguish between 'Missing' values when the number is not applicable, versus when the number was truly not present in the set |\n",
    "| case_onset_interval | Missing Data | Impute using k-nearest neighbor algorithm |\n",
    "| case_onset_interval | Outliers | Keep as is for now, after cleaning the data, re-evaluate |\n",
    "| process | 90%+ Missing | Drop Feature |\n",
    "| exposure_yn | 90%+ Missing | Drop Feature \n",
    "| current_status | Illogical Data | Replace with 'Laboratory-confirmed case' if 'case_positive_specimen_interval' is not Null |\n",
    "| symptom_status | Illogical Data | Replace with 'Yes' if 'hospital_yn' is 'Yes', 'icu_yn' is 'Yes', or 'death_yn' is 'Yes'; Replace with 'Yes if 'case_onset_interval' is not Null |\n",
    "| symptom_status | Missing Data | Impute using k-nearest neighbor algorithm |\n",
    "| hosp_yn | Illogical Data | Replace with 'Yes' if 'icu_yn' is 'Yes |\n",
    "| hosp_yn | Missing Data | Impute using k-nearest neighbor algorithm |\n",
    "| icu_yn | 90%+ Missing | Drop Feature |\n",
    "| underlying_conditions_yn | 90%+ missing | Drop Feature |\n",
    "\n",
    "Additionally, a number of external features from beyond the CDC data set have been included in the dataset:\n",
    "- Income: the average household income for residents in the patient's county\n",
    "- People_Vaccinated: the number of Americans who had been vaccinated at the time of the patient's illness\n",
    "- Risk_Factor: a flag to indicate whether the patient is in one of the groups with more deaths than others in the original dataset (Age 50-64 or 65+, Male, Black, or Hispanic/Latino).\n",
    "- Month_of_Year: the month in which a patient's illness is reported, irrespective of the year.\n",
    "- Season: the season in which a patient's illness is reported.\n",
    "More information on these features can be found in the notebook for Task 1.\n",
    "\n",
    "Finally, the data from the previous assignment is imported below, with two files, one for each student. We use pandas to concatenate the two sets into one large dat set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_student_01 = pd.read_csv('data\\covid19-cdc-22202398_final_assignment2.csv', keep_default_na=True)\n",
    "data_student_02 = pd.read_csv('data\\covid19-cdc-18389156_final_assignment2.csv', keep_default_na=True)\n",
    "\n",
    "data = pd.concat([data_student_01, data_student_02], axis=0)\n",
    "\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To recap what has already been done, we will check the shape of the set, inspect the datatypes and check if there are any remaining invalid null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows, columns = data.shape\n",
    "print(f\"The data set has {rows} rows and {columns} columns.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 Assigning DataTypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data types we have assigned in the data cleaning step were not preserved when the file was converted to a .csv. As such, we will need to reintroduce them before starting to work with the data. First we will assign ints and datetime types to the relevant features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get continuous date features and int features separately\n",
    "continuous_date = ['case_month']\n",
    "continuous_int = ['case_positive_specimen_interval', 'case_onset_interval', 'income', 'people_vaccinated', 'month_of_year']\n",
    "\n",
    "# Get aggregated continuous features\n",
    "continuous = continuous_date + continuous_int\n",
    "\n",
    "# Assign types to columns - dates\n",
    "for column in continuous_date:\n",
    "    data[column] = data[column].astype('datetime64[ns]')\n",
    "\n",
    "# Assign types to columns - integers\n",
    "for column in continuous_int:\n",
    "    data[column] = data[column].astype('Int64')\n",
    "\n",
    "# Print data types\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will assign data types to the categorical features. We will separate these features into two categories: boolean and category.  \n",
    "$~$  \n",
    "**Boolean Features**\n",
    "- current_status: True for Laboratory Confirmed Case, False for Probably Case\n",
    "- symptom_status: True for Symptomatic, False for Asymptomatic\n",
    "- hosp_yn: \n",
    "- risk_factor: True for 1, False for 0\n",
    "- detah_yn: True for y, False for n  \n",
    "\n",
    "**Category Features**  \n",
    "- county_fips_code (*this feature is perhaps too granular, so we will replace it with state fips code*)\n",
    "- age_group\n",
    "- sex\n",
    "- race\n",
    "- ethnicity\n",
    "- season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical features\n",
    "categorical_category = ['county_fips_code', 'age_group', 'sex', 'race', 'ethnicity', 'season']\n",
    "categorical_bool = ['current_status', 'symptom_status', 'hosp_yn', 'risk_factor', 'death_yn']\n",
    "categorical =  categorical_category + categorical_bool\n",
    "\n",
    "# Assign types to columns - boolean\n",
    "for column in categorical_bool:\n",
    "    if column == 'current_status':\n",
    "        data[column] = data[column].map({'Laboratory-confirmed case': 1, 'Probable Case': 0})\n",
    "    elif column == 'symptom_status':\n",
    "        data[column] = data[column].map({'Symptomatic': 1, 'Asymptomatic': 0})\n",
    "    elif column == 'risk_factor':\n",
    "        data[column] = data[column].astype(bool)\n",
    "    elif column == 'hosp_yn' or column == 'death_yn':\n",
    "        data[column] = data[column].map({'Yes': 1, 'No': 0})\n",
    "    elif column == 'risk_factor':\n",
    "        data[column] = data[column].map({1: 1, 0: 0})\n",
    "    data[column] = data[column].astype('int64')\n",
    "\n",
    "# Assign types to columns - categorical\n",
    "for column in categorical_category:\n",
    "    data[column] = data[column].astype('category')\n",
    "\n",
    "# Rename columns to match restructuring of data\n",
    "data.rename(columns={'current_status': 'laboratory_confirmed_tf', 'symptom_status': 'symptomatic_tf', 'hosp_yn': 'hosp_tf', 'risk_factor': 'risk_factor_tf', 'death_yn': 'death_tf'}, inplace=True)\n",
    "\n",
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\63051593.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Update county_fips_code column, converting county fips codes to state fips codes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'county_fips_code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'county_fips_code'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;33m//\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'uint8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'county_fips_code'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;34m'state_fips_code'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# Get renamed categorical features\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Update county_fips_code column, converting county fips codes to state fips codes\n",
    "data['county_fips_code'] = data['county_fips_code'].apply(lambda row: row // 1000).astype('uint8')\n",
    "data.rename(columns={'county_fips_code': 'state_fips_code'}, inplace=True)\n",
    "\n",
    "# Get renamed categorical features\n",
    "categorical_category = ['state_fips_code', 'age_group', 'sex', 'race', 'ethnicity', 'season']\n",
    "categorical_bool = ['laboratory_confirmed_tf', 'symptomatic_tf', 'hosp_tf', 'risk_factor_tf', 'death_tf']\n",
    "categorical =  categorical_category + categorical_bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 Checking Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the 'case_positive_specimen_interval' and 'case_onset_interval' features contain null values, which are logical considering the nature of the data. To incorporate these features in the regression models, we have several options for handling the null values.\n",
    "\n",
    "We can immediately eliminate the option of dropping the features due to the small percentage of null values. To address the null values in each column, we will instead consider two approaches: imputation with representative values and replacement with a distinct value. \n",
    "1. **Imputation**: We will explore three strategies - imputing with the median, mean, and mode values of the non-null data.\n",
    "2. **Replacement**: We will also consider replacing the null values with a distinct value that significantly exceeds the range of actual values, such as -1 or 999.\n",
    "To determine the best approach, we will assess the correlation of each imputation method and the replacement approach with the target feature. By considering their impact on the target feature, we can make an informed decision on how to handle the null values in the two columns.\n",
    "\n",
    "#### Case Positive Specimen Interval\n",
    "We will first implement the two approaches for the case_positive_specimen_interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address null values for case_positive_specimen_interval\n",
    "data_case_pos_spec_na= data.copy()\n",
    "\n",
    "# Option 1.1: Impute with median\n",
    "data_case_pos_spec_na['case_pos_spec_median'] = data['case_positive_specimen_interval'].fillna(data['case_positive_specimen_interval'].median()).astype('int64')\n",
    "# Option 1.2: Impute with mean\n",
    "data_case_pos_spec_na['case_pos_spec_mean'] = data['case_positive_specimen_interval'].fillna(data['case_positive_specimen_interval'].mean().round()).astype('int64')\n",
    "# Option 1.1: Impute with mode\n",
    "data_case_pos_spec_na['case_pos_spec_mode'] = data['case_positive_specimen_interval'].fillna(data['case_positive_specimen_interval'].mode().values[0]).astype('int64')\n",
    "\n",
    "# Option 2: Impute with strange value 999\n",
    "data_case_pos_spec_na['case_pos_spec_odd_value'] = data['case_positive_specimen_interval'].fillna(999).astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationHeatmap(data_case_pos_spec_na, ['case_positive_specimen_interval', 'case_pos_spec_median', 'case_pos_spec_mean', 'case_pos_spec_mode', 'case_pos_spec_odd_value', 'death_tf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviewing the correlations above, it is evident that the differences in the correlations with the target feature are minimal. The variations observed between the original column with null values and the columns where imputation was performed are very slight, with none of the differences exceeding 0.007. \n",
    "\n",
    "|Replacement Column|correlation with death_tf|difference in correlation|\n",
    "|:----|:----|:----|\n",
    "|case_positive_specimen_interval|0.02|-|\n",
    "|case_pos_spec_median|0.019|0.001|\n",
    "|case_pos_spec_mean|0.019|0.001|\n",
    "|case_pos_spec_mode|0.019|0.001|\n",
    "|case_pos_spec_odd_value|0.013|0.007|\n",
    "\n",
    "We conclude that the imputation process had a negligible impact on the correlations between the features and the target variable. We will choose the option that is closest to the original and drop the original column. In this case mean, median, and mode all present with the same correlation and we will arbitrarily choose to use the mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned = data.copy()\n",
    "data_cleaned['case_positive_specimen_interval'] = data_case_pos_spec_na['case_pos_spec_mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Case Onset Interval\n",
    "We will perform the same steps for the 'case_onset_interval' feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Address null values for case_positive_specimen_interval\n",
    "data_case_onset_na= data.copy()\n",
    "\n",
    "# Option 1.1: Impute with median\n",
    "data_case_onset_na['case_onset_median'] = data['case_onset_interval'].fillna(data['case_onset_interval'].median()).astype('int64')\n",
    "# Option 1.2: Impute with mean\n",
    "data_case_onset_na['case_onset_mean'] = data['case_onset_interval'].fillna(data['case_onset_interval'].mean().round()).astype('int64')\n",
    "# Option 1.1: Impute with mode\n",
    "data_case_onset_na['case_onset_mode'] = data['case_onset_interval'].fillna(data['case_onset_interval'].mode().values[0]).astype('int64')\n",
    "\n",
    "# Option 2: Impute with strange value 999\n",
    "data_case_onset_na['case_onset_odd_value'] = data['case_onset_interval'].fillna(999).astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotCorrelationHeatmap(data_case_onset_na, ['case_onset_interval', 'case_onset_median', 'case_onset_mean', 'case_onset_mode', 'case_onset_odd_value', 'death_tf'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, the differences in the correlations with the target feature are minimal. With the exception of the case_onset_odd_value, none of the differences exceeds 0.001. \n",
    "\n",
    "|Replacement Column|correlation with death_tf|difference in correlation|\n",
    "|:----|:----|:----|\n",
    "|case_onset_interval|0.053|-|\n",
    "|case_onset_median|0.054|-0.001|\n",
    "|case_onset_mean|0.054|-0.001|\n",
    "|case_onset_mode|0.054|-0.001|\n",
    "|case_onset_odd_value|-0.068|0.121|\n",
    "\n",
    "We conclude that the imputation process had a negligible impact on the correlations between the features and the target variable. Again mean, median, and mode are equally correlated with the target feature, and we choose to use the mode for consistency. We can now drop the original column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaned['case_onset_interval'] = data_case_onset_na['case_onset_mode']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these changes, we can verify that null values are no longer present in the set and proceed to splitting the data for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_cleaned' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\3145182753.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_cleaned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'data_cleaned' is not defined"
     ]
    }
   ],
   "source": [
    "data_cleaned.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that there are no null values, we will convert all of the continuous columns from Int64 (nullable int64) to int64 (non-nullable int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign types to columns - integers\n",
    "for column in continuous_int:\n",
    "    data_cleaned[column] = data_cleaned[column].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Splitting the data into two data sets\n",
    "\n",
    "We can now split the data into two datasets. We have elected not to shuffle the data set because sklearn's split function will shuffle it before splitting. **70%** of the data will be used for training and **30%** will be use for testing.\n",
    "\n",
    "We will set the random_state variable to 0, to allow the random shuffle to be repeated within this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset into a pandas dataframe and set X and y\n",
    "X = data_cleaned[['case_month', 'state_fips_code', 'age_group', 'sex', 'race', 'ethnicity', 'case_positive_specimen_interval', 'case_onset_interval', 'laboratory_confirmed_tf', 'symptomatic_tf', 'hosp_tf', 'income', 'people_vaccinated', 'risk_factor_tf', 'month_of_year', 'season']]\n",
    "y = data_cleaned['death_tf']\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Concatenate the X and y training variables into a table.\n",
    "data_training = pd.concat([X_train, y_train], axis=1)\n",
    "\n",
    "# Concatenate the X and y testing variables into a table.\n",
    "data_testing = pd.concat([X_test, y_test], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To be as transparent as possible when we split the data, we print the range of rows in the original table, the training table, and the testing table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original range is:\\t rows 0 to\", data_cleaned.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(data_training.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(data_training.shape[0]), \"to\", round(data_training.shape[0]) + data_testing.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Start working with the training set\n",
    "\n",
    "To get a better understanding of the data we are working with, we will analyze the features in our dataset. We will achieve this by analyzing relationships between both continuous and categorical features, both among themselves and with the target feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.3.1 Continuous Features\n",
    "***\n",
    "#### **1.3.1.1 Continuous - Continuous Plots**\n",
    "\n",
    "First we plot and examine the correlations between continuous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the target feature to the list of continuous columns\n",
    "continuous_corr_features = continuous + ['death_tf']\n",
    "\n",
    "# Print the correlation between the continuous data types\n",
    "getCorrelations(data_training, continuous_corr_features, abs_corr=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the correlation table\n",
    "plotCorrelationHeatmap(data_training, continuous_corr_features, abs_corr=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the results\n",
    "The plots allow us to visualize pairs of features which correlate strongly to each other. We have arbitrarily identified three types of correlations: low correlation, medium correlation, and high correlation. The boundaries for each bin are arbitrary and used solely for classification purposes.\n",
    "\n",
    "**Low correlation (x <= 0.02)**\n",
    "All but 1 pair of features have low correlation. This is to be expected, since it indicates that these features operate independently and do not have a significant impact on each other. Despite their weaker associations, these features still provide unique information that can contribute to the predictive models, offering valuable insights into other factors impacting the target variable.  \n",
    "\n",
    "**Medium correlation (0.02 < x < 0.10)**  \n",
    "There quite a few pairs of features with medium correlation. All of the features are at least slightly correlated to case_month and people_vaccinated, while many of the features also seem to have some correlation with income. \n",
    "\n",
    "**High correlation (x >= 0.10)**  \n",
    "Of our few continuous features, there are two pairs with strong correlations. 'people_vaccinated' and 'case_month' have a correlation of 0.94. This is to be expected, since as time passes, more and more people are vaccinated. We can choose to drop one of these features, since they will trend in a similar direction, but as vaccination rates plateau and case months continue to grow, it may be beneficial to keep the two parameters separate. Keeping them distinct allows us to capture the temporal aspect and understand the evolving relationship between vaccinations and cases. The other two are 'people_vaccinated' and 'month_of_year'. This one also makes sense, since they are both linked to the temporal aspect of the progression of the virus.\n",
    "$~$  \n",
    "\n",
    "**Features to Remove:** For now, we decide to keep all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1.2 Continuous - Target Feature Scatter Plots**\n",
    "We will create scatter plots to compare each continuous feature with the target feature. This visual analysis will help us identify the features that have a stronger potential to predict the target variable and distinguish them from the less relevant features. By examining these scatter plots, we can isolate a subset of features that appear to be more indicative, allowing us to focus our modeling efforts on the most relevant variables. Features with weaker associations can be eliminated to streamline the model and reduce noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(continuous_corr_features)\n",
    "fig, axs = plt.subplots(ncols=1, nrows=n-1, figsize=(10,40))\n",
    "\n",
    "# dict to hold correlation values \n",
    "corr_dict = {}\n",
    "\n",
    "target_feature = 'death_tf'\n",
    "\n",
    "for i, feature in enumerate(continuous_corr_features):\n",
    "    if feature != target_feature:\n",
    "        xlabel = feature.replace(\"_\", \" \").title()\n",
    "        ylabel = target_feature.replace(\"_\", \" \").title()\n",
    "\n",
    "        correlation = data_training[[feature, target_feature]].corr(numeric_only=False).values[0,1]\n",
    "        title = xlabel + \" vs \" + ylabel\n",
    "        values = data_training[[feature, target_feature]]\n",
    "        scatter = values.plot.scatter(x=feature, y=target_feature, ax=axs[i], grid=True, fontsize=10)\n",
    "        scatter.legend([\"Correlation: {:.2f}\".format(abs(correlation))])\n",
    "        scatter.set_xlabel(xlabel)\n",
    "        scatter.set_ylabel(ylabel)\n",
    "        scatter.set_title(title)\n",
    "\n",
    "        # add correlation to dict\n",
    "        corr_dict[feature] = abs(correlation)\n",
    "\n",
    "# dataframe holding sorted correlation values to aid in interpreting results\n",
    "corr_df = pd.DataFrame.from_dict(corr_dict, orient='index', columns=['RiskPerformance']).sort_values('RiskPerformance', ascending=False)\n",
    "corr_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the results\n",
    "After analyzing the plots, it is evident that there is no single feature that exhibits a very strong correlation with the target feature. This aligns with our understanding of the complexity of predicting survival from COVID-19. The outcome is influenced by a multitude of factors, and no single feature can serve as a definitive indicator of survival.\n",
    "\n",
    "We again categorized the correlation values into three bins: low correlation, medium correlation, and high correlation. The boundaries for each bin are arbitrary and used solely for classification purposes.\n",
    "\n",
    "\n",
    "| Low correlation | Medium correlation | High correlation |\n",
    "|-----------------|--------------------|------------------|\n",
    "| x <= 0.02       | 0.02 < x < 0.10    | x >= 0.10       |\n",
    "| 2 features    | 2  features        | 2 features       |\n",
    "| 33%             | 33%                | 33%              | \n",
    "\n",
    "\n",
    "**Low correlation (x <= 0.02)**  \n",
    "There are two features that demonstrate low correlations, but we have decided to retain them for now. Removing these features would result in the elimination of a significant portion of the dataset. We will further consider this decision is in the next step, and evaluate the impact of this decision in Part 5.  \n",
    "\n",
    "**Medium correlation (0.02 < x < 0.10)**  \n",
    "There are two features with medium correlation: 'case_onset_interval' and 'month_of_year'\n",
    "\n",
    "**High correlation (x >= 0.10)**  \n",
    "The strongest correlations are observed between 'case_month'/'people_vaccinated' and 'death_tf'. This correlation is expected as both features are closely related to the progression of time and the impact of vaccinations. The strong correlations of these features with 'death_tf' align with our expectations, as the duration of the pandemic is likely to capture the complex factors contributing to the severity of the virus.  \n",
    "  \n",
    "$~$  \n",
    "**Features to Remove:** For now, we decide to keep all of the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1.3 Continuous - Target Feature Histograms Plots**\n",
    "Lastly, we will generate a set of stacked histograms comparing the histograms. While the scatter plots provide insights into the strength of the correlations, the stacked histograms offer a different perspective by illustrating the distribution of deaths across different values of the continuous features. Although the stacked histograms do not directly depict the correlation between the features and the target, they serve a crucial purpose in examining the patterns of deaths within each feature category. This analysis allows us to identify any notable variations or trends in the distribution, which may indicate a potential association between certain feature values and the likelihood of deaths.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=1, nrows=6, figsize=(15,50))\n",
    "fig.subplots_adjust(hspace=0.35)\n",
    "\n",
    "for i, column in enumerate(continuous):\n",
    "    if column == 'case_positive_specimen_interval' or column == 'case_onset_interval':\n",
    "        createStackedHistograms(data_training, 'death_tf', column, bin_count = 14, ax=axs[i], filterExp=lambda row: row[column] <= 14, ylim=2)\n",
    "    else:\n",
    "        createStackedHistograms(data_training, 'death_tf', column, bin_count = 10, ax=axs[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the Results\n",
    "\n",
    "After analyzing the plots, we have made three more conclusions on the data.\n",
    "\n",
    "- The 'case_onset_interval' column has an overwhelming majority of values that are 0, and the distribution of deaths (Yes and No) is almost even for each value of this attribute, with the difference within 1%. \n",
    "\n",
    "- In contrast, 'case_positive_specimen_interval', despite also having a majority of values as 0, shows a correlation between deaths and its value. When the 'case_positive_specimen_interval' value is 0, the amount of deaths vs survivors is 79% vs 89%. However, for the next two values, the percentages flip, and the amount of deaths vs survivors is 11% vs 8%, and 9% vs 1%.\n",
    "\n",
    "- 'income', although exhibiting a slight correlation with death, provides a diverse distribution of data, which may lead to interesting results.\n",
    "\n",
    "- 'month_of_year', although barely exhibiting a correlation with death, has an interesting correlation with the total number of cases.\n",
    "\n",
    "**Features to Remove:** We decide to remove 'case_onset_interval'.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.1.4 Selection of Indicative Continuous Features**\n",
    "The set of indicative continuous features we will be using includes the following:\n",
    "* case_positive_specimen_interval\n",
    "* income\n",
    "* case_month\n",
    "* people_vaccinated\n",
    "* month_of_year\n",
    "\n",
    "Based on the histogram analysis and correlation statistics, we've decided to exclude 'case_onset_interval' due to its overwhelming majority of zero values and the even distribution of deaths for each value of this attribute. On the other hand, we've decided to retain 'case_positive_specimen_interval'. Even though the majority of its values are also 0, there is a noticeable correlation between deaths and the feature's value.\n",
    "\n",
    "Income has been included as an indicative feature because, despite its slight correlation with death, the variable's diverse distribution could yield interesting insights.\n",
    "\n",
    "Lastly, 'case_month' and 'people_vaccinated' have been included due to the strong correlations identified in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_low_correlation_features = ['case_onset_interval' ]\n",
    "# 'case_month']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 1.3.2 Categorical Features\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.2.1 Categorical - Categorical Plots**\n",
    "Although we cannot simply create a correlation heatmap between the categorical features, we can create a cross tab heatmap for each pair of categorical features. This will enable us to visualize strong correlations between pairs of feature, and decide whether to keep and remove them accordingly. This type of heat map provides a visual representation for interactions between pairs of values for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'categorical' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\2268043679.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mncols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m70\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumn_i\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcategorical\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'categorical' is not defined"
     ]
    }
   ],
   "source": [
    "n = len(categorical)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=n, nrows=n, figsize=(100, 70))\n",
    "\n",
    "for i, column_i in enumerate(categorical):\n",
    "    for j, column_j in enumerate(categorical):\n",
    "        if j < i:\n",
    "            plotCrosstabHeatmap(data_training, column_i, column_j, axs[i][j])\n",
    "        else:\n",
    "            axs[i][j].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the results\n",
    "The plots allow us to visualize pairs of features which correlate strongly to each other; areas of the heatmap which are red correspond indicate that there are many patients which fall into that combination of categories, and areas which are cyan correspond to very few. However, there are a few limitations to this visualization, namely when it comes to sample size. If the sample size for one category (e.g., 'White' in the 'Race' feature) is significantly larger than others, it might dominate the heatmap. This makes it appear as though there are stronger associations for that category, even if those are merely due to the larger sample size. Because of this we will simply use this as a tool to understand distribution, rather than a guide for the prediction model.\n",
    "\n",
    "**Observations**\n",
    "- Some patient groups have more representation than others. These observations should match the analysis of the data from Task 1.\n",
    "    - There are many more 'white' patients than non-white.\n",
    "    - There are many more 'non-hispanic/latino' than 'hispanic/latino' patients.\n",
    "    - There are many more 'laboratory-confirmed' patients than not.\n",
    "    - There are many more 'symptomatic' patients than not.\n",
    "    - There are fewer 'hospitlized' patients than not\n",
    "- By design there is a correlation between risk factor and some of the other demographic groups. We will keep all of these features for now, but a potential optimization in Part 5 will be to drop either the demographic features or the risk factor feature.\n",
    "\n",
    "**Features to Remove:** Keeping in mind the limitations stated on this visualization, we will not decide to remove any features based solely on this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.2.2 Categorical - Target Plots**\n",
    "<li style=\"color: cyan; font-style: italic\">For each categorical feature, plot its pairwise interaction with the target feature. Discuss what  knowledge you gain from these plots, e.g., which categorical features seem to be better at predicting the target feature? </li>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(categorical)-1\n",
    "\n",
    "fig, axs = plt.subplots(ncols=1, nrows=n, figsize=(6, 60))\n",
    "\n",
    "for i, feature in enumerate(categorical):\n",
    "    if feature != 'death_tf':\n",
    "        createStackedBarChart(data_training, 'death_tf', feature, axs=axs[i] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Interpretation of the results\n",
    "\n",
    "The stacked bar charts seem to be better indicators of relationships between the features. We will briefly evaluate the relationship between each of the categorical features and death.\n",
    "\n",
    "**State vs Death**\n",
    "- Observations of the data reveal an unclear pattern between the state and the occurrence of death. However, it's noteworthy that in 12 states, over 50% of the COVID-19 cases in the study resulted in death.\n",
    "- This could be due to various factors such as differing state-level responses, demographics, or access to healthcare facilities.\n",
    "- Despite the lack of a clear pattern, we will retain this feature due to the potential for hidden relationships that might be discovered with further analysis.\n",
    "\n",
    "**Age Group vs Death**\n",
    "- There's a strong correlation between age group and death. The death rate for the 0-17 and 18-49 age groups is below 0.01%, but this increases dramatically to 15% for those aged 50-64 and surges to 68% for those aged 65 and above.\n",
    "- This correlation is to be expected, since it is well known that age is a risk factor for death.\n",
    "- We will definitely retain this feature due to its strong correlation with the target feature.\n",
    "\n",
    "**Sex vs Death**\n",
    "- There appears to be a slight correlation between sex and death. The death rate is 21% for females and 28% for males.\n",
    "- We will retain this feature, as even slight correlations have the potential to improve the accuracy of the model.\n",
    "\n",
    "**Race vs Death**\n",
    "- There's a considerable range in the percentage of cases leading to death among different racial groups. The death rate for white patients is 26%, decreasing to 19% for black patients, 17% for Asian patients, 14% for others, 13% for American Indian/Alaska Natives, and 0% for Native Hawaiian.\n",
    "- The observed variation might be related to the sample size or other social and health factors.\n",
    "- We will retain this feature because of the potential for uncovering deeper insights.\n",
    "\n",
    "**Ethnicity vs Death**\n",
    "- A significant difference in the occurrence of death between Hispanic/Latino and non-Hispanic/Latino patients is observed, with Hispanic/Latino patients dying more often by 12%.\n",
    "- We will retain this feature as it shows a notable difference in death rates.\n",
    "\n",
    "**Season vs Death**\n",
    "- There appears to be a correlation between season and percent of cases leading to death. The percentage ranges from 20% in Summer to +10% in Spring. Autumn and Winter are somewhere in between.\n",
    "- We will retain this features as it shows a slight correlation, which has the potential to improve the accuracy of the model.\n",
    "\n",
    "**Laboratory Confirmed Case vs Death**\n",
    "- The data shows that the proportion of patients who died with a laboratory-confirmed case is almost equal to the proportion of patients who died without a confirmed case - 24% and 26% respectively.\n",
    "- This suggests that laboratory confirmation does not significantly impact the likelihood of death.\n",
    "- Since the feature does not seem to provide valuable predictive power, we will not retain this feature.\n",
    "\n",
    "**Symptomatic vs Death**\n",
    "- The data reveals a significant correlation between being symptomatic and death, as no deaths were observed in asymptomatic patients.\n",
    "- Given the absolute correlation, we will definitely retain this feature.\n",
    "\n",
    "**Hospitalized vs Death**\n",
    "- Another strong correlation is seen with hospitalization. Patients who were hospitalized died 69% of the time, compared to an 8% death rate for non-hospitalized patients.\n",
    "- This is expected because hospitalization is typically indicative of more severe disease.\n",
    "- We will retain this feature due to its strong correlation with the target feature.\n",
    "\n",
    "**Risk Factor vs Death**\n",
    "- The 'Risk Factor' feature, which is an aggregate of several other features, shows a notable correlation with death. No deaths were observed in patients with a risk factor of 0, while a death rate of 30% was noted among patients with a risk factor of 1.\n",
    "- However, because this feature is a composite of several others, including it might introduce multicollinearity into our model. For this reason, we will include this feature for the moment, but consider eliminating it or the features it encapsulates in later optimization steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **1.3.2.3 Selection of Indicative Categorical Features**\n",
    "The set of indicative categorical features we will be using includes the following:\n",
    "* state_fips_code\n",
    "* age_group\n",
    "* sex\n",
    "* ethnicity\n",
    "* symptomatic_tf\n",
    "* hosp_tf\n",
    "* risk_factor_tf\n",
    "* season\n",
    "\n",
    "The only categorical feature to be dropped will be laboratory_confirmed_case_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_low_correlation_features = ['laboratory_confirmed_tf']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.3 Summary & Drop Features\n",
    "\n",
    "1. The continuous features to be dropped, identified in 1.3.1.4 are as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cont_low_correlation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "2. The categorical features to be dropped, identified in 1.3.2.3 are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_low_correlation_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now drop these features\n",
    "- Before dropping the features, we will duplicate the original dataframe, data_cleaned, and label it model_data\n",
    "- We will drop the features from model_data, allowing us to revert back to the original dataset when we want to compare the performance of our new dataset with the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the data frame\n",
    "dropped_column_data = data_cleaned.copy()\n",
    "# Concatenate the list of features to drop\n",
    "features_to_drop = cont_low_correlation_features + cat_low_correlation_features\n",
    "# Drop the features\n",
    "dropped_column_data = dropped_column_data.drop(columns=features_to_drop)\n",
    "\n",
    "# Print the new dataframe\n",
    "dropped_column_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Prepare Dataset for Modeling\n",
    "\n",
    "We have now identified the features that we want to use in our predictive models.To prepare them for modeling, we will need to convert all categorical features into dummy variables or integers to allow modeling. We will be using one-hot/dummy encoding for sex, race, ethnicity, and season, and we will use integer encoding for age_group (where it makes sense to preserve the order of categories). The remaining categorical features have already been converted to binary 0 or 1 values, and will not need to be converted for modeling, but we will convert them to uint8 types for memory efficiency. The other integer encoding we will need to do is converting the datetime variable for case_month into a representative integer.\n",
    "\n",
    "### 1.4.1 Integer encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = dropped_column_data.copy()\n",
    "\n",
    "# Integer encoding of age group\n",
    "model_data['age_group'] = model_data['age_group'].map({'65+ years': 3, '50 to 64 years': 2, '18 to 49 years': 1, '0 - 17 years': 0}).astype('uint8')\n",
    "\n",
    "# List boolean values in our dataset to be converted to uint8, then converted\n",
    "model_data_boolean_values = ['symptomatic_tf', 'hosp_tf', 'death_tf']\n",
    "model_data[model_data_boolean_values] = model_data[model_data_boolean_values].astype('uint8')\n",
    "\n",
    "# Print datatypes\n",
    "model_data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Integer encoding of case_month column\n",
    "# Get first case month\n",
    "min_date = model_data['case_month'].min()\n",
    "# Subtract min_date from each date in 'case_month', the result will be a datetime.timedelta object\n",
    "model_data['case_month'] = model_data['case_month'] - min_date\n",
    "# Convert the timedelta object to number of days and then to months\n",
    "model_data['case_month'] = (model_data['case_month'] / np.timedelta64(1, 'M')).astype('int64')\n",
    "\n",
    "# Print datatypes\n",
    "model_data.dtypes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 Dummy Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\3772932160.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mmodel_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_data' is not defined"
     ]
    }
   ],
   "source": [
    "model_data = pd.get_dummies(model_data)\n",
    "model_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Redundant Features\n",
    "We do not need to keep sex_Male or ethnicity_Non-Hispanic/Latino, since these implied with the falseness of sex_Female and ethnicity_Hispanic/Latino respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data.drop(columns=['sex_Male','ethnicity_Non-Hispanic/Latino'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.3 Summary\n",
    "#### Continuous Columns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_columns = model_data.select_dtypes(include=['int64']).columns.tolist()\n",
    "continuous_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Categorical Columns are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = model_data.select_dtypes(include=['uint8']).columns.tolist()\n",
    "categorical_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Total features are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = continuous_columns + categorical_columns\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.4 Train/Test Split\n",
    "Repeat the process of splitting the data into testing and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset into a pandas dataframe and set X and y\n",
    "features.remove('death_tf')\n",
    "# descriptive_features = ['case_positive_specimen_interval', 'income', 'people_vaccinated', 'age_group', 'symptomatic_tf', 'hosp_tf', 'sex_Female',  'ethnicity_Hispanic/Latino', 'month_of_year']\n",
    "target_feature = ['death_tf'] \n",
    "\n",
    "X = model_data[features]\n",
    "y = model_data[target_feature]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "print(\"original range is:\\t rows 0 to\", data_cleaned.shape[0])\n",
    "print(\"training range (70%):\\t rows 0 to\", round(data_training.shape[0]))\n",
    "print(\"test range (30%): \\t rows\", round(data_training.shape[0]), \"to\", round(data_training.shape[0]) + data_testing.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the X_train printout below that the indexes are no longer consecutive. We need to reset the indices in order to enable merging of the datasets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\3048016215.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "# need to reset the index to allow contatenation with predicted values otherwise not joining on same index...\n",
    "X_train.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)\n",
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.5 Normalize the Dataset\n",
    "The final step we'll take before beginning to model is to normalize the dataset so that all values are between 0 and 1. This will ensure our features are weighted equally, and also make our coefficients more digestible and informative. We will use sklearn's MinMaxScaler and fit the scaler using only the training data. This will prevent data leakage between the testing and training data. Then we will scale both the training and the testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit on the training data\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transform the testing and training data\n",
    "X_train = pd.DataFrame(scaler.transform(X_train), columns = X_train.columns)\n",
    "X_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)\n",
    "\n",
    "# Also ravel the y_test and y_train for ease of use later on\n",
    "y_train = y_train.values.ravel()\n",
    "y_test = y_test.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets are now ready for modeling!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictiveModel(model, X_data: pd.DataFrame, y_data: pd.DataFrame, threshold: float = 0.5):\n",
    "    \"\"\"\n",
    "    Trains a model on the provided data, makes predictions, thresholds these predictions based on the provided \n",
    "    threshold, and displays a DataFrame of actual vs. predicted values.\n",
    "\n",
    "    Parameters:\n",
    "    model (BaseEstimator): The scikit-learn model to train.\n",
    "    X_data (DataFrame): The input data for the model.\n",
    "    y_data (DataFrame or Series): The target data for the model.\n",
    "    threshold (float): The threshold for deciding predictions. Default is 0.5.\n",
    "\n",
    "    Returns:\n",
    "    BaseEstimator: The trained model.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_data, y_data)\n",
    "    \n",
    "    # Calculate the prediction and threshold the value if it's not a classifier\n",
    "    if isinstance(model, (LinearRegression,)):\n",
    "        predictions = (model.predict(X_data) >= threshold) * 1.0\n",
    "    else:\n",
    "        predictions = model.predict(X_data)\n",
    "\n",
    "    # Create a DataFrame with actual vs. predicted values\n",
    "    actual_vs_predicted = pd.concat([pd.DataFrame(y_data, columns=['Actual']), pd.DataFrame(predictions, columns=['Predicted'])], axis=1)\n",
    "\n",
    "    # Show the head of the table\n",
    "    display(actual_vs_predicted.head(100))\n",
    "\n",
    "    # Return the trained model\n",
    "    return model\n",
    "\n",
    "def evaluate_classification(data_type, actual_classification, predicted_classification):\n",
    "    \"\"\"\n",
    "    Evaluate classification results and display metrics.\n",
    "\n",
    "    Args:\n",
    "        data_type (str): Type of data ('Test' or 'Training').\n",
    "        actual_classification (array-like): Actual classification values.\n",
    "        predicted_classification (array-like): Predicted classification values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Display data type\n",
    "    display(HTML(f\"<span style='font-weight:bold; font-size: larger;'>{data_type} Data</span>\"))\n",
    "    \n",
    "    # Accuracy\n",
    "    accuracy = metrics.accuracy_score(actual_classification, predicted_classification)\n",
    "    display(HTML(f\"<strong>Accuracy</strong>: {round(accuracy, 4)}\"))\n",
    "    \n",
    "    # Confusion matrix\n",
    "    display(HTML(f\"<strong>Confusion Matrix</strong>\"))\n",
    "    cm = metrics.confusion_matrix(actual_classification, predicted_classification)\n",
    "    display(HTML(f\"<div style='margin-left:auto; margin-right: auto;'>{tabulate(pd.DataFrame(cm), headers=['Predicted Survival', 'Predicted Death'], showindex=['Actual Survival', 'Actual Death'], tablefmt='html')}</div>\"))\n",
    "    \n",
    "    # Classification report\n",
    "    display(HTML(f\"<strong>Classification Report</strong>\"))\n",
    "    report = metrics.classification_report(actual_classification, predicted_classification, output_dict=True)\n",
    "    report.pop('accuracy')\n",
    "    display(HTML(f\"<div style='margin-left:auto; margin-right: auto;'>{tabulate(pd.DataFrame(report).transpose(), headers='keys', tablefmt='html')}</div>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2.0 Predictive Modeling: Linear Regression** \n",
    "\n",
    "## 2.1 Train the Linear Regression Model\n",
    "<span style=\"color: cyan; font-style:italic;\">On the training set, train a linear regression model to predict the target feature, using only the  descriptive features selected in exercise (1) above. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_linreg = LinearRegression().fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Interpret the Linear Regression Model\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "- Linear regression is a modeling tool that is used to make predictions based on linear relationship between the target (dependent variable) and any number of predictors (independent variables)\n",
    "    - It finds the line of best fit the describes the relationship between the target and predictors \n",
    "    - This line is calculated by minimising the overall error\n",
    "- The linear regression formula takes the following form:\n",
    "    - $target\\_feature = w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n $\n",
    "    - The output of this formula will be a continuous value that can be less than 0 and higher than 1\n",
    "- We can see the calculated intercept is -0.86 (w_0)\n",
    "    - This is the starting point. i.e. if all other coefficients are zero then the model result will be -0.86\n",
    "    - Can be thought of as where the model line intercepts the y axis\n",
    "- We can see all the coefficients for each feature\n",
    "    - These are zipped together in a single list for ease of inspection.\n",
    "    - The sum of all the feature*coefficients + intercept will result in the model prediction  \n",
    "- We cannot make a direct comparison based on the value of the coefficients as it is tied directly to the range of each feature.\n",
    "    - If all features were normalized this would be possible to see directly\n",
    "    - However if we multiply each feature by its max range we can get an idea of its max possible impact on the prediction.  We could look at median also.\n",
    "    - For ExternalRiskEstimate its max is 93 (93 * 0.0147 = 1.367)\n",
    "    - For PercentTradesNeverDelq its max is 100 (100 * -0.0024 = 0.24)\n",
    "    - From this we can see that ExternalRiskEstimate can have a bigger impact than PercentTradesNeverDelq. \n",
    "    - This should correspond roughly with the correlation values from part 1   \n",
    "    \n",
    "It is important to note that the output from a linear regression model is not suited to the classification problem that we are trying to solve.\n",
    "- The output is not a probability and an additional thresholding step is necessary to convert the output into a binary classification\n",
    "- We will threshold the output so that any values >=0.5 will be cast to 1, any values <0.5 will be cast to 0\n",
    "\n",
    "Finally it is worth mentioning the effect outliers can have on linear regression output. \n",
    "- Consider the effect of the outlier in the graph below on the regression line and what values fall into each threshold category. \n",
    "- The outlier will have a huge effect on values near the threshold. \n",
    "- This will be discussed further in the next section. <br>\n",
    "<img src=\"linear_regression_outliers.png\"> <br>\n",
    "source: https://medium.com/@rgotesman1/learning-machine-learning-part-3-logistic-regression-94db47a94ea3\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "### **2.2.1 Coefficients**\n",
    "<span style=\"color: cyan; font-style: italic;\">  Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature).  </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "display(HTML(tabulate([('Intercept:', multiple_linreg.intercept_)], tablefmt='html')))\n",
    "\n",
    "# Display the DataFrame as an HTML table\n",
    "# Create a list of tuples with features and coefficients\n",
    "feature_coefficients = list(zip(X_train.columns, multiple_linreg.coef_))\n",
    "\n",
    "# Sort the list by coefficients in ascending order\n",
    "sorted_feature_coefficients = sorted(feature_coefficients, key=lambda x: abs(x[1]))\n",
    "\n",
    "# Display the sorted table using tabulate\n",
    "display(HTML(tabulate(sorted_feature_coefficients, headers=['Features', 'Coefficients'], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.2.2 Target Feature Values and Predicted Class**\n",
    "<span style=\"color: cyan; font-style: italic;\">  Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples.</span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "- Here we will print the predicted target feature value for the first 100 training examples. \n",
    "- We will threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. \n",
    "    - If value is >= 0.5 it is cast to 1, if < 0.5 it is cast to 0\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the prediction and threshold the value. If >= 0.5 it's true\n",
    "multiple_linreg_predictions_train = (multiple_linreg.predict(X_train) >= 0.5) * 1.0\n",
    "\n",
    "# Create a DataFrame with actual vs. predicted values\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([pd.DataFrame(y_train, columns=['Actual']), pd.DataFrame(multiple_linreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression:\")\n",
    "actual_vs_predicted_multiplelinreg.head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Evaluate the Linear Regression Model\n",
    "\n",
    "### **2.3.1 Classification Evaluation Measures on Training Set**\n",
    "<span style=\"color: cyan; font-style: italic;\">   Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1)</span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "- We will print the classification evaluation measures computed on the training set (e.g. Accuracy, Confusion matrix, Precision, Recall, F1)\n",
    "- We will discuss findings based on these measures\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification('Train', y_train, multiple_linreg_predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of the Results\n",
    "\n",
    "- Accuracy\n",
    "    - This is simply stating how often the model is correct. We have an accuracy of 89%\n",
    "- Confusion Matrix\n",
    "    - [0][0] TRUE NEGATIVE - The number that is predicted 0 that was actually 0\n",
    "    - [0][1] FALSE POSITIVE - The number that is predicted 1 that is actually 0\n",
    "    - [1][0] FALSE NEGATIVE - The number that is predicted 0 that is actually 1\n",
    "    - [1][1] TRUE POSITIVE - The number that is predicted 1 that is actually 1\n",
    "- Precision - How good the model is at predicting the positive class\n",
    "    - What % of the predicted positive are actually positive\n",
    "    - It is the number values correctly predicted positive over the total number of  positive values\n",
    "    - Precision Positive is 0.83\n",
    "    - Precision Negative is 0.92\n",
    "- Recall\n",
    "    - What % of the positive values did we predict\n",
    "    - Is the number correctly predicted positive over the total number actual positive\n",
    "    - Recall Positive is 0.75\n",
    "    - Recall Negative is 0.95\n",
    "- F1 Score\n",
    "    - Is an a weighted average of Precision and recall\n",
    "    - F1 Score Positive is 0.79\n",
    "    - F1 Score Negative is 0.94\n",
    "\n",
    "#### Summary\n",
    "- Theses values seem reasonable.\n",
    "- The model is a notably better at predicting the negative class. (good)\n",
    "- This makes sense as the model has more negative data to learn from - it is the majority class. \n",
    "- When predicting life and death, accuracy becomes much more significant. However, for sake of a patient's happiness it is preferrable for them to believe in their survival. Overestimating survival is preferrable to underestimating it.\n",
    "- The downside is if individuals die who thought they would survive, it can be harder for their loved ones to come to terms with their death\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2.3.2 Classification Evaluation Measures on the Testing Set**\n",
    "<span style=\"color: cyan; font-style: italic\"> Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. </span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "- The results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e. a new model trained and evaluated using cross-validation on the full dataset).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_linreg_predictions_test = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelinreg = pd.concat([pd.DataFrame(y_test, columns=['Actual']), pd.DataFrame(multiple_linreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "\n",
    "actual_vs_predicted_multiplelinreg.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification('Test', y_test, multiple_linreg_predictions_test)\n",
    "evaluate_classification('Train', y_train, multiple_linreg_predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of the Results\n",
    "- Accuracy\n",
    "    - The accuracy of the test data is marginally higher 90.41% vs 90.3%.\n",
    "- Precision\n",
    "    - The precision score for predicting the positive case has remained at 83%.\n",
    "    - The precision score for predicting the negative case has increased from 92% to 93%.\n",
    "- Recall\n",
    "    - The recall score for predicting the positive case has increased from 75% to 76%.\n",
    "    - The recall score for predicting the negative case has  remained at 95%.\n",
    "- F1\n",
    "    - The f1 score for predicting the positive case has increased from 79% to 80%.\n",
    "    - The f1 score for predicting the negative case has remained at 94%.\n",
    "\n",
    "#### Summary\n",
    "- These values are a little higher than expected as we are now testing the model prediction on data it has not seen before\n",
    "- This is a good sign that the model is generalising\n",
    "- More comparisons need to be made and this is where cross validation steps in \n",
    "\n",
    "### **2.3.3 Classification Evaluation Measures on Cross-Validated Set**\n",
    "<span style=\"color: cyan; font-style: italic\"> Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated random train/test (70/30) splits. </span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "- We will now perform cross validation on the linear regression model. \n",
    "- Here we perform the same evaluation as above but multiple times\n",
    "- Each time the data is shuffled so we get a slightly different view of the data for training and testing\n",
    "- This works well for evaluating on a limited set of data\n",
    "- We will store the results in a dictionary for later use\n",
    "\n",
    "First we need to create a function to perform this cross validation. Sklearn does not provide one for linear regression. However it does for logistic and random forests models\n",
    "- Cross validation Function can be seen below\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg(X, y, cv=10, scoring='accuracy'):\n",
    "    \"\"\"Functions to carry out cross validation on the linear regression model\n",
    "    Default number of validations is 10. The random state will be updated \n",
    "    at each iteration to allow our results to be repeated\"\"\"\n",
    "    \n",
    "    # store results\n",
    "    results = []\n",
    "    # evaluate cv times and append to results\n",
    "    for i in range(cv):\n",
    "        # set up train test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=i , test_size=0.3)\n",
    "        # generate model\n",
    "        multiple_linreg = LinearRegression().fit(X_train, y_train)\n",
    "        # threshold\n",
    "        multiple_linreg_predictions = (multiple_linreg.predict(X_test) >= 0.5) * 1.0\n",
    "        # calc score\n",
    "        if scoring=='accuracy':\n",
    "            score = metrics.accuracy_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='precision':\n",
    "            score = metrics.precision_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='f1':\n",
    "            score = metrics.f1_score(y_test, multiple_linreg_predictions)\n",
    "        elif scoring=='recall':\n",
    "            score = metrics.recall_score(y_test, multiple_linreg_predictions)\n",
    "        # append to results\n",
    "        results.append(score)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "We will now create an additional function to perform 10 fold cross validation and store results into a dataframe\n",
    "- This will be used to simplify further analysis the dataset, looking at accuracy, precision, recall, f1.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LinReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    linRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "    \n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_LinReg(X, y, cv=10, scoring=metric)\n",
    "        length = len(result)\n",
    "        # store result in dict\n",
    "        linRegResults[metric] = sum(result)/length\n",
    "\n",
    "    # create dataframe with results\n",
    "    LinRegDF = pd.DataFrame.from_dict(linRegResults, orient='index', columns=['Linear_Regression'])\n",
    "    \n",
    "    return LinRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "Perform 10 fold cross validation using cross_val_LinReg_DF function\n",
    "- Results summarized below\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\2137152018.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinRegDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_LinReg_DF\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Mean results from 10 fold cross validation are:\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlinRegDF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "linRegDF = cross_val_LinReg_DF(X,y)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "linRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Interpretation of the Results\n",
    "<span style=\"color: cyan; font-style: italic\"> Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings. </span>\n",
    "\n",
    "<div style=\"color: lightgreen\">These results are marginally lower than previous results but this is expected. We have taken the mean of 10 sets of results. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3.0 Predictive Modeling: Logistic Regression** \n",
    "\n",
    "## 3.1 Train the Logistic Regression Model\n",
    "<span style=\"color: cyan; font-style:italic;\">On the training set, train a logistic regression model to predict the target feature, using only the  descriptive features selected in exercise (1) above. </span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply oversampling using SMOTE\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_log, y_train_log = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train aka fit, a model using all continuous and categorical features.\n",
    "multiple_logisticreg = LogisticRegression().fit(X_train_log, y_train_log)\n",
    "\n",
    "X_train_log.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 3.2 Interpret the Logistic Regression Model\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "The logistic regression function is a little different than the linear regression function. \n",
    "- The output of the logistic function is a probability, a value between 0 and 1\n",
    "- The output of the linear function is a continuous value that is not a probability\n",
    "- Differences aside there is a direct relationship between the two\n",
    "    - The first part of logistic regression function is similar to linear regression i.e. We find the line of best fit\n",
    "    - We then pass this equation through what is called a sigmoid function\n",
    "    - This sigmoid function will output a value bound between 0 and 1. It is a probability\n",
    "    - The model then applies a threshold to this probability so that if is is >= 0.5 its cast to 1 and if it is <0.5 it is cast to 0\n",
    "    - All of these steps are carried out within the logistic regression function, however the threshold value can be adjusted up or down depending on the problem you are trying to solve.\n",
    "\n",
    "The model estimated in logistic regression is given by the logistic function: <br>\n",
    "$probability(target=1|descriptive\\_features)=logistic(w_0 + w_1 * feature_1 + w_2*feature_2 + ...+ w_n*feature_n)$ <br>\n",
    "where $logistic(x)$ is defined as: $logistic(x) = \\frac{e ^ x}{1 + e ^ x} = \\frac{1}{1+e^{-x}}$\n",
    "\n",
    "- From the values above can see the calculated intercept is -1.006\n",
    "    - This is the starting point. i.e. if all other coefficients were zero then the value for 'x' would be -1.006. \n",
    "    - This is the input to the logistic function and the logistic function will then calculate the probability and threshold based on this.\n",
    "- We can see all the coefficients for each features\n",
    "    - These are zipped together in a single list for ease of inspection.\n",
    "    - The effect of these on the value 'x' is the same as for linear regression\n",
    "    - Only when the value 'x' is fed into the logistic function do we see the real difference between the to methods.\n",
    "    \n",
    "A major benefit of logistic regression worth  highlighting is its ability to handle outliers. \n",
    "- As discussed for linear regression, outliers can significantly skew what values fall within each threshold point. \n",
    "- The graph below highlights the minimal effect outliers can have on logistic regression model.\n",
    "- Values near the threshold point will not be impacted significantly<br>\n",
    "<img src=\"logistic_regression_outliers.png\"> <br>\n",
    "Source: https://towardsdatascience.com/understanding-logistic-regression-9b02c2aec102\n",
    "\n",
    "</div>\n",
    "\n",
    "### **3.2.1 Coefficients**\n",
    "<span style=\"color: cyan; font-style: italic;\">  Print the coefficients learned by the model and discuss their role in the model (e.g., interpret the model by analysing each coefficient and how it relates each input feature to the target feature).  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the weights learned for each feature.\n",
    "display(HTML(tabulate([('Intercept:', multiple_logisticreg.intercept_)], tablefmt='html')))\n",
    "\n",
    "# Display the DataFrame as an HTML table\n",
    "# Create a list of tuples with features and coefficients\n",
    "feature_coefficients = list(zip(X_train_log.columns, multiple_logisticreg.coef_.ravel()))\n",
    "\n",
    "# Sort the list by coefficients in ascending order\n",
    "sorted_feature_coefficients = sorted(feature_coefficients, key=lambda x: abs(x[1]))\n",
    "\n",
    "# Display the sorted table using tabulate\n",
    "display(HTML(tabulate(sorted_feature_coefficients, headers=['Features', 'Coefficients'], tablefmt='html')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **3.2.2 Target Feature Values and Predicted Class**\n",
    "<span style=\"color: cyan; font-style: italic;\">  Print the predicted target feature value for the first 10 training examples. Threshold the predicted target feature value given by the linear regression model at 0.5, to get the predicted class for each example. Print the predicted class for the first 10 examples.</span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "- Here we will print the predicted target feature value for the first 100 training examples based on training data\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_train_log = (multiple_logisticreg.predict(X_train_log)>= 0.5) * 1.0\n",
    "multiple_logisticreg_predictions_train = (multiple_logisticreg.predict(X_train)>= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple linear regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([pd.DataFrame(y_train_log, columns=['Actual']), pd.DataFrame(multiple_logisticreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "\n",
    "actual_vs_predicted_multiplelogisticreg.head(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Evaluate the Logistic Regression Model\n",
    "### **3.3.1 Classification Evaluation Measures on Training Set**\n",
    "<span style=\"color: cyan; font-style: italic;\">   Print a few classification evaluation measures computed on the full training set (e.g., Accuracy, Confusion matrix, Precision, Recall, F1)</span>\n",
    "\n",
    "<div style=\"color:lightgreen\"> \n",
    "\n",
    "- We will print the classification evaluation measures computed on the training set (e.g. Accuracy, Confusion matrix, Precision, Recall, F1)\n",
    "- We will discuss finding based on these measures\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_classification('Oversampled Train', y_train_log, multiple_logisticreg_predictions_train_log)\n",
    "evaluate_classification('Not Oversampled Train', y_train, multiple_logisticreg_predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Results\n",
    "<span style=\"color: cyan; font-style: italic\"> discuss your findings so far. </span>\n",
    "\n",
    "<div style=\"color:lightgreen\">\n",
    "\n",
    "\n",
    "Interpretation of results\n",
    "- Accuracy\n",
    "    - We have an accuracy of 74%\n",
    "- Precision - How good model is a prediction the positive class\n",
    "    - Precision Positive is 0.72\n",
    "    - Precision Negative is 0.76\n",
    "- Recall - What % of the positive values did we predict\n",
    "    - Recall Positive is 0.68\n",
    "    - Recall Negative is 0.79\n",
    "- F1 Score - Is an a weighted average of Precision and recall\n",
    "    - F1 Score Positive is 0.70\n",
    "    - F1 Score Negative is 0.77\n",
    "\n",
    "\n",
    "Summary\n",
    "- These values seem reasonable\n",
    "- Accuracy is in line with the linear regression model\n",
    "- The logistic model is also a little better at predicting the negative class.\n",
    "</div>\n",
    "\n",
    "### **3.3.2 Classification Evaluation Measures on the Testing Set**\n",
    "<span style=\"color: cyan; font-style: italic\"> Evaluate the model using classification evaluation measures on the hold-out (30% examples) test set. Compare these results with the evaluation results obtained on the training (70%) dataset. </span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "\n",
    "- The results from the test data will be compared the results from the training data.\n",
    "- In addition they will be compared to the results from a cross-validated model (i.e. a new model trained and evaluated using cross-validation on the full dataset).\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "multiple_logisticreg_predictions_test = (multiple_logisticreg.predict(X_test)>= 0.5) * 1.0\n",
    "\n",
    "print(\"\\nPredictions with multiple logistic regression: \\n\")\n",
    "actual_vs_predicted_multiplelogisticreg = pd.concat([pd.DataFrame(y_test, columns=['Actual']), pd.DataFrame(multiple_logisticreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "actual_vs_predicted_multiplelogisticreg.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some more evaluation metrics.\n",
    "evaluate_classification('Test', y_test, multiple_logisticreg_predictions_test)\n",
    "evaluate_classification('Oversampled Train', y_train_log, multiple_logisticreg_predictions_train_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interpretation of Results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3.3.3 Classification Evaluation Measures on Cross-Validated Set**\n",
    "<span style=\"color: cyan; font-style: italic\"> Also compare these results with a cross-validated model (i.e., a new model trained and evaluated using cross-validation on the full dataset). You can use classic k-fold cross-validation or repeated random train/test (70/30) splits. </span>\n",
    "\n",
    "<div style=\"color: lightgreen\">\n",
    "We will first create function to perform 10 fold cross validation and store results into dataframe\n",
    "- This will be used to simplify further analysis the dataset, looking at accuracy, precision, recall, f1.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_LogReg_DF(X,y):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    logRegResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(LogisticRegression(), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        logRegResults[metric] = result.mean()\n",
    "        \n",
    "    # create dataframe with results\n",
    "    LogRegDF = pd.DataFrame.from_dict(logRegResults, orient='index', columns=['Logistic_Regression'])\n",
    "    \n",
    "    return LogRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preform 10 fold cross validation using cross_val_LogReg_DF function\n",
    "- Results summarized below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_log = pd.concat([X_train_log, X_test], axis=0)\n",
    "y_log = pd.concat([pd.DataFrame(y_train_log, columns=['death_tf']), pd.DataFrame(y_test, columns=['death_tf'])], axis=0)\n",
    "\n",
    "logRegDF = cross_val_LogReg_DF(X_log,y_log)\n",
    "print(f\"Mean results from 10 fold cross validation are:\")\n",
    "logRegDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Interpretation of the Results\n",
    "<span style=\"color: cyan; font-style: italic\"> Compare the cross-validation metrics to those obtained on the single train/test split and discuss your findings. </span>\n",
    "\n",
    "<div style=\"color: lightgreen;\">\n",
    "These results are in line with the previous results. We have taken the mean of 10 sets of results. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4.0 Predictive Modeling: Random Forest** \n",
    "\n",
    "## 4.1 Train the Random Forest Model\n",
    "\n",
    "We will now create and train a Random Forest model to predict our target feature. To do this we will use sklearn's RandomForestClassifier. We set the number of estimators (number of trees in the forest) to 100 and oob_score to true to use the unused validation data to score the accuracy of the random forest model. Setting the random_state to 1 will make the model deterministic, and we will be able to reproduce the results on subsequent runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train RF with 100 trees\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_features='auto', oob_score=True, random_state=1)\n",
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Interpret the Random Forest Model\n",
    "\n",
    "### **4.2.1 Understanding the Random Forest Model**\n",
    "Like the regression models, Random Forest is a supervised algorithm used for predicting target features based on a set of predictive features. Where they differ is their approach to predicting this data. The random forest model uses an ensemble of decision trees, each of which structures the data in a hierarchical form based on feature values. A decision tree is made up of a root node (or the starting node), interior nodes, and leaf nodes (or the terminating nodes). At each non-leaf node, a test is carried out that will split the feature space into two distinct regions, determining which of the children nodes the process should follow. This process is then repeated until a leaf node is reached, at which point a prediction can be made. The test that is carried out is an if-then-else rule that is meant to split the data based on the highest information gain. This process is similar to a human guessing game, which makes decision trees easy to understand and interpret, while capturing complex relationships between features.  \n",
    "$~$  \n",
    "Again, a random forest is an ensemble of decision trees, each of which taking its own approach to creating its decision-making hierarchy. Each decision tree in the ensemble uses a different sample of data points and features (selected through the process of bagging and subspace sampling) to ensure randomness and diversity in each of the decision trees. Once every tree arrives at a prediction, they come to a consensus on the final predicted value. This is either through a majority vote in the case of categorical target features or selecting the median in the case of continuous features.\n",
    "\n",
    "<img src=\"./RandomForestModel.png\" width=\"500px\">\n",
    "\n",
    "A few other definitions to help us understand the output of the model are listed below:\n",
    "- Information Gain - a measure of the reduction in overall entropy of a set of instances that is achieved by testing on a specific feature\n",
    "- Entropy - the level of disorder or uncertainty in a dataset\n",
    "- Information Gain Ratio - alternative to entropy-based calculation of information gain; computed by dividing the information gain of a feature by the amount of information used to determine the value of the feature.\n",
    "- Gini Index - measures how well a particular split separates the data into homogeneous subsets based on their class labels or target values.This value is always between 0 and 1, where 0 means that all data points in the dataset have the same target value, and 1 means that they are perfectly heterogenous.\n",
    "\n",
    "We will use the Random Forest model to predict whether or not patients will die of Covid considering their descriptive features.\n",
    "To start, we will look at an example of a single decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4.2.2  Training the Decision Tree**\n",
    "We will train two decision trees of different depths. One will have a max depth of 4 and the other will have a max depth of 10. This parameter sets the allowable number of levels of the hierarchy before it must give an answer. Otherwise the model would run until all of the nodes are leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a decision tree classifier with a max depth of 4, and a random state of 1 for reproducibility\n",
    "dtc4 = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
    "# Train the decision tree model\n",
    "dtc4.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate another decision tree classifier with a max depth of 4, and a random state of 1 for reproducibility\n",
    "dtc4v2 = DecisionTreeClassifier(max_depth=4, random_state=5)\n",
    "# Train the decision tree model\n",
    "dtc4v2.fit(X_train, y_train)\n",
    "\n",
    "# Instantiate a decision tree classifier with a max depth of 10, and a random state of 1 for reproducibility\n",
    "dtc10 = DecisionTreeClassifier(max_depth=6, random_state=1)\n",
    "# Train the decision tree model\n",
    "dtc10.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  **4.2.3  Display the Decision tree**\n",
    "We will create graph of each tree and store as an image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a Graphviz png\n",
    "with open(\"DecisionTree4.dot\", 'w') as f1:\n",
    "    f1 = export_graphviz(dtc4, out_file=f1, feature_names=X_train.columns)\n",
    "with open(\"DecisionTree4v2.dot\", 'w') as f2:\n",
    "    f2 = export_graphviz(dtc4v2, out_file=f2, feature_names=X_train.columns)\n",
    "with open(\"DecisionTree10.dot\", 'w') as f3:\n",
    "    f3 = export_graphviz(dtc10, out_file=f3, feature_names=X_train.columns)\n",
    "\n",
    "d4tree = open('DecisionTree4.dot', 'r').read()\n",
    "graph4 = pydotplus.graph_from_dot_data(d4tree)\n",
    "graph4.write_png('DecisionTree4.png')\n",
    "\n",
    "d4treev2 = open('DecisionTree4v2.dot', 'r').read()\n",
    "graph4v2 = pydotplus.graph_from_dot_data(d4treev2)\n",
    "graph4v2.write_png('DecisionTree4v2.png')\n",
    "\n",
    "d10tree = open('DecisionTree10.dot', 'r').read()\n",
    "graph10 = pydotplus.graph_from_dot_data(d10tree)\n",
    "graph10.write_png('DecisionTree10.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two decision trees with max depth 4 and one more with max depth of 10 are shown below. A couple of observations from the trees:\n",
    "- The root node tends to be age_group <= 0.83, which corresponds to age_group < 65+\n",
    "- This test has a gini, or information gain index of 0.365, which means although it is a good test, neither child node will contain a homogenous group of features.\n",
    "- The next test that is checked in all three trees is whether the patient has been hospitalized. \n",
    "- Leaf nodes are reached after 4 splits of the data in trees with a max depth of 4, which corresponds to the max depth parameters we've set, but in the tree with a max depth of 10, leaf nodes are reached as soon as 4 splits, and no path requires over 6 splits to reach a leaf node. Identifying the features it is using to make these decisions can give us some insights into the importance those features.\n",
    "- It is surprising the impact of the state_fips_code, and perhaps misleading. The model is treating the state_fips_codes as a continuous feature and using them to make decisions based on their sequential order, perhaps indicating that this is a candidate for removal when the model is optimized.\n",
    "\n",
    "##### Decision Tree 4 v1\n",
    "<img src=\"DecisionTree4.png\">\n",
    "\n",
    "##### Decision Tree 4 v2\n",
    "<img src=\"DecisionTree4v2.png\">\n",
    "\n",
    "##### Decision Tree 10\n",
    "<img src=\"DecisionTree10.png\">\n",
    "\n",
    "### **4.2.4 Interpretation of the random forest model**\n",
    "For each prediction, the random forest model will synthesize the 100 decision trees and determine the most common outcome given the values of the record being predicted. The information from the decision trees themselves will allows us to extract which features are most important from the dataset, based on the gini index at each node for each tree. The importance values are printed below, and will allow us to cull features to optimize the model for speed and efficiency in future steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame({'feature': X_train.columns, 'importance':rfc.feature_importances_})\n",
    "importance.sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The list above allows us to see which values are of the most important. It appears that the most important features are age_group (which we already guessed based on the root node of the decision trees we visualized), hosp_tf, state_fips_code, and income. This aligns with some of the features already identified as important in the linear regression, and overlaps with some of the values of the logistic regression. It is important to note that no values have extremely high importance indices. However there are many features with surprisingly low importance, including nearly all of the race features and the seasonal features. These features are candidates for removal in later optimizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Evaluate the Random Forest Model\n",
    "\n",
    "### **4.3.1 Classification Evaluation Measures on the Training Set**\n",
    "The first step in evaluating the random forest model is to print out the actual values from the training data and the predicted values. We will also print out the classification measures, the accuracy, the confusion matrix, and the precision, recall, F-1 score, and support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model, on in-sample data (same sample used for training and test), print actual vs predicted values\n",
    "rfc_predictions_train = rfc.predict(X_train)\n",
    "df_true_vs_rfc_predicted = pd.DataFrame({'ActualClass': y_train, 'PredictedClass': rfc_predictions_train})\n",
    "\n",
    "# Display actual vs predicted\n",
    "display(df_true_vs_rfc_predicted.head(100))\n",
    "# Display classification measures\n",
    "evaluate_classification('Train', y_train, rfc_predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has achieved exceptional precision, recall, and f1-scores of 99%. While these results may seem impressive due to the extensive features and deep structure used, it is important to be aware of potential overfitting. The risk of overfitting suggests the need to carefully evaluate the model's generalization on the testing set, as it may result in lower accuracy metrics compared to the training data.  \n",
    "### **4.3.2 Classification Evaluation Measures on the Testing Set**\n",
    "To really test the predicted model, we apply it to the testing data. We will print out the actual vs predicted class, as well as some evaluation measures, comparing them to the training evaluation measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the trained model, on in-sample data (same sample used for training and test), print actual vs predicted values\n",
    "rfc_predictions_test = rfc.predict(X_test)\n",
    "df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "df_true_vs_rfc_predicted_test.head(20)\n",
    "\n",
    "# Display actual vs predicted\n",
    "display(df_true_vs_rfc_predicted_test.head(100))\n",
    "\n",
    "# Display classification measures\n",
    "evaluate_classification('Train', y_train, rfc_predictions_train)\n",
    "evaluate_classification('Test', y_test, rfc_predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the provided data, the evaluation measures reveal that the random forest model exhibits an impressive 93% accuracy. This means that it successfully predicted the outcomes of 93% of the patients in the testing set. Notably, the model demonstrates exceptional precision and recall of 95% for correctly identifying the \"no death\" cases. In other words, when the model predicts \"no death,\" it is accurate 95% of the time and can capture the majority of \"no death\" instances. However, when it comes to identifying \"death\" cases, the precision and recall drop slightly to 86%. Although these values are slightly lower, they are still respectable and indicate the model's ability to correctly classify \"death\" cases in 86% of its predictions while capturing 86% of the actual \"death\" cases.\n",
    "\n",
    "Additionally, both the f1-scores and the harmonic mean of precision and recall exhibit similar values for both classes. This consistency is expected since the f1-score represents the balance between precision and recall. Given the similarity in these measures, it can be inferred that the model maintains a balanced performance across both the \"no death\" and \"death\" predictions.\n",
    "\n",
    "It's worth noting that the class distribution in the training set reveals a significant class imbalance, as evidenced by the support column (20190 vs 6368). The relatively lower prevalence of the \"death\" class in the training data might have contributed to the slightly lower precision and recall for this class. To address this issue and potentially enhance the model's predictive performance for \"death\" cases, an optimization approach could involve utilizing an oversampled dataset during the training phase. By generating synthetic examples of the minority class, the model would have access to additional information, enabling it to learn more effectively and make better predictions regarding the likelihood of death.\n",
    "### **4.3.3 Classification Evaluation Measures on Cross-Validated Set**\n",
    "Another way to check the accuracy of the model is to compare the results with a cross-validated model. We will use k-fold cross-validation, which splits the model into k different datasets and averages the classification evaluation measures for each split. It provides a more reliable estimate of model performance because it ensures that the model is tested against a diverse and random set of samples of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_val_RandomForest_DF(X,y, depth=None, estimators=100):\n",
    "    \"\"\"Function to perform cross validation and store results \n",
    "    in dataframe. Cross validation looks at accuracy, precision, \n",
    "    recall, f1. Returns a dataframe with results\"\"\"\n",
    "\n",
    "    # store results in dict\n",
    "    RandomForestResults = {}\n",
    "    # metrics to test against\n",
    "    test_metrics = ['accuracy','precision','recall', 'f1']\n",
    "\n",
    "    for metric in test_metrics:\n",
    "        # generate test results\n",
    "        result = cross_val_score(RandomForestClassifier(n_estimators=estimators, max_features='auto', oob_score=True, random_state=1, max_depth=depth), X, y, scoring=metric, cv=10)\n",
    "        # store result in dict\n",
    "        RandomForestResults[metric] = result.mean()\n",
    "    \n",
    "    # create dataframe with results\n",
    "    RandomForestDF = pd.DataFrame.from_dict(RandomForestResults, orient='index', columns=['Random_Forests'])\n",
    "\n",
    "    return RandomForestDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results for Random Forests are summarized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RandomForestDF = cross_val_RandomForest_DF(X,y)\n",
    "display(HTML(f\"<strong>Mean results from 10-fold cross validation</strong>\"))\n",
    "display(RandomForestDF)\n",
    "\n",
    "# compute the out-of-bag classification accuracy\n",
    "display(HTML(f\"<strong>Out of Bag Score:</strong> {rfc.oob_score_}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results indicate that the average accuracy of the model is about the same as it was when evaluated on the testing dataset, or at about 93%. Just like in the evaluation of the training and testing data sets, precision and recall values specifically for True outcomes of patients are lower. As previously mentioned, the under-representation of patients who died in the dataset impacts the accuracy of these values. To address this issue, oversampling techniques could be implemented, providing the decision trees with more information about patients at risk of COVID-related death. Despite this limitation, the model correctly predicting positive cases 85% of the time that it predicts positive (precision) and identifying patients who actually died 87% of the time (recall).\n",
    "\n",
    "Additionally, the Out of Bag (OOB) score, which estimates the model's performance on unseen data during training, is calculated to be 92.9%. This means that testing on samples that were left out of the individual decision trees averaged to 92.9%. This is in line with the accuracy of the testing, training, and cross validated sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Interpretation of the Results\n",
    "Building on the initial analysis, our random forest model predicting COVID-19 patient outcomes exhibits pretty good performance with an accuracy of 93% on the testing set. The model demonstrates remarkable precision and recall for correctly identifying \"no death\" cases, demonstrating accuracy and completeness of 95%. However, for \"death\" cases, these metrics reduce to 86%, a potential area for future optimization.\n",
    "\n",
    "The f1-scores, which represent the harmonic mean of precision and recall, provide similar readings for both classes, indicating a balanced performance. Despite this, the model shows a class imbalance in the training set, with \"death\" cases underrepresented, potentially leading to the lower precision and recall for this class.\n",
    "\n",
    "The model's accuracy remains consistent when using k-fold cross-validation, averaging at around 93%. The Out of Bag (OOB) score, used to estimate the model's performance on unseen data during training, aligns closely with this, calculated to be 92.9%.\n",
    "\n",
    "In terms of future optimizations, several approaches can be considered. Reducing the maximum depth of the decision trees could help mitigate potential overfitting and improve the model's generalizability. The feature 'state_fips_code' seems to be falsely interpreted as a continuous variable by the model, which can potentially skew the results, making it might potentially beneficial to remove this feature in future iterations. Finally, other features with lower importance could also be eliminated to simplify the model and improve its efficiency without significantly affecting its predictive performance. Applying these adjustments would streamline our model, potentially enhancing its predictive capabilities and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5.0 Optimizations & Conclusion**\n",
    "\n",
    "To establish a baseline for comparison, we will create a simple model which always predicts the majority class, and print out its confusion matrix and classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_16648\\658549560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmajority_class\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0msimple_majority\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mmajority_class\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mevaluate_classification\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Simple Majority'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msimple_majority\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'y_test' is not defined"
     ]
    }
   ],
   "source": [
    "majority_class = 0\n",
    "simple_majority = [majority_class] * len(y_test)\n",
    "\n",
    "evaluate_classification('Simple Majority', y_test, simple_majority)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 5.1 Model Comparison and Justification\n",
    "Three models were built to predict COVID-19 outcomes: linear regression, logistic regression, and a random forest model. All three models demonstrated acceptable performance with the logistic regression model outperforming the others in terms of precision and recall after oversampling the minority class. This was an important step, given the class imbalance in the data, where the number of survivors vastly outnumbered the fatalities. The random forest model, however, showed the highest accuracy. The table below summarizes accuracy, prediction, recall, and f1-score for the 3 models, and compares them with the values for the simple majority (using the false values rather than positive). All three models perform better in all accuracy and precision than simply taking the simple majority.\n",
    "\n",
    "| |Simple Majority (Invert)|Linear Regression|Logistic Regression|Random Forest|\n",
    "|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.754173|0.901327|0.915343|0.931444|\n",
    "|precision|0.754173|0.814833|0.879845|0.850861|\n",
    "|recall|1|0.765406|0.937446|0.868752|\n",
    "|f1|0.859862|0.789335|0.907706|0.859583|\n",
    "\n",
    "We expect that with similar oversampling steps, the random forest model could achieve comparable precision and recall metrics to the logistic regression model. For this reason, we will say that the random forest model performs the best, and verify this assumption in our optimization steps.\n",
    "\n",
    "\n",
    "## 5.2 Understanding of the Problem and Predictive Modeling Results\n",
    "The predictive modeling results revealed some valuable insights into the features contributing to COVID-19 outcomes. Age_group, hospitalization, symptom_status, people_vaccinated, and state_fips_code were identified as important features across all models. However, some issues were noted. The state_fips_code was treated as a continuous variable when it should have been handled as a categorical variable. Additionally, the inclusion of hospitalization is questionable as this information may not be available at the time of contracting COVID-19.\n",
    "\n",
    "Class imbalance was also identified as a potential issue, with a significantly higher number of survivors than fatalities. This imbalance can lead to biased models with lower precision and recall for the minority class - in this case, patient death. Thus, the models had less information to correctly predict patient death, leading to less-than-optimal predictive performance for this category.\n",
    "\n",
    "## 5.3 Ideas for Improvement and Implementation\n",
    "\n",
    "### 5.3.1 Random Forest Optimization  \n",
    "We will write a function that will allows us to modify the random forest model and view the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_random_forest(X_train, y_train, n_estimators=100, max_features='auto', max_depth=None, X_test=X_test):\n",
    "    # Train RF with specified number of trees and max features\n",
    "    rfc = RandomForestClassifier(n_estimators=n_estimators, max_features=max_features, max_depth=max_depth, oob_score=True, random_state=1)\n",
    "    rfc.fit(X_train, y_train)\n",
    "\n",
    "    # Show the feature importance\n",
    "    importance = pd.DataFrame({'feature': X_train.columns, 'importance': rfc.feature_importances_})\n",
    "    display(importance.sort_values('importance', ascending=False))\n",
    "\n",
    "    # Using the trained model, on in-sample data (same sample used for training and test), print actual vs predicted values\n",
    "    rfc_predictions_train = rfc.predict(X_train)\n",
    "    df_true_vs_rfc_predicted_train = pd.DataFrame({'ActualClass': y_train, 'PredictedClass': rfc_predictions_train})\n",
    "\n",
    "    # Using the trained model, on out-of-sample data (test set), print actual vs predicted values\n",
    "    rfc_predictions_test = rfc.predict(X_test)\n",
    "    df_true_vs_rfc_predicted_test = pd.DataFrame({'ActualClass': y_test, 'PredictedClass': rfc_predictions_test})\n",
    "\n",
    "    # Display classification measures\n",
    "    evaluate_classification('Train', y_train, rfc_predictions_train)\n",
    "    evaluate_classification('Test', y_test, rfc_predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.1.1 Optimization 1 - Addressing Class Imbalance\n",
    "**Oversampling**\n",
    "\n",
    "Our first approach to optimizing the model involves oversampling the minority class. This technique is designed to address the class imbalance in our data. We will try an undersampling the dominant target feature in a step down below. For now we will assess the accuracy precision and recall of the random forest model before and after oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_opt1, y_train_opt1 = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt1, y_train_opt1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Undersampling**\n",
    "\n",
    "As an alternative to oversampling, we also explore undersampling the majority class to tackle the issue of class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Create an instance of the RandomUnderSampler\n",
    "rus = RandomUnderSampler()\n",
    "\n",
    "X_train_opt1_2, y_train_opt1_2 = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt1_2, y_train_opt1_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)|\n",
    "|:----|:----|:----|:----|\n",
    "|accuracy|0.931444|0.9324| 0.9258|\n",
    "|precision|0.850861|0.844248| 0.790428|\n",
    "|recall|0.868752|0.889207| 0.950322|\n",
    "|f1|0.859583|0.866144| 0.863031|\n",
    "\n",
    "The overall accuracy and precision of the model have decreased, but recall and f1 have both increased. In the case of COVID-19, recall may be the more important factor, where we are more concerned with over-predicting death and taking steps to try to prevent it, so we determine that the oversampled model performs better.\n",
    "\n",
    "#### 5.3.1.2 Optimization 2 - Set Maximum Depth\n",
    "\n",
    "For our third optimization, we will limit the maximum depth of the model. This strategy may not enhance accuracy, but it aims to streamline the model by reducing its complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_opt2 = X_train_opt1.copy()\n",
    "y_train_opt2 = y_train_opt1.copy()\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt2, y_train_opt2, max_depth = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)| Optimization 2 |\n",
    "|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.9314|0.9324| 0.9258|0.9185\n",
    "|precision|0.850861|0.844248| 0.790428| 0.779271|\n",
    "|recall|0.868752|0.889207| 0.950322|0.932452|\n",
    "|f1|0.859583|0.866144| 0.863031| 0.849007|\n",
    "\n",
    "\n",
    "#### 5.3.1.3 Optimization 3 - Remove Unnecessary Features\n",
    "\n",
    "Our fourth optimization involves eliminating features that are not influential in our model's predictions. We categorize features with an importance value of less than 0.05 as unimportant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('hosp', 'race', 'season', 'sex', 'case_positive_specimen_interval', 'income', 'ethnicity', 'month_of_year'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_opt3 = X_train_opt2.drop(columns=columns_to_drop)\n",
    "y_train_opt3 = y_train_opt2.copy()\n",
    "X_test_opt3 = X_test.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt3, y_train_opt3, max_depth = 4, X_test = X_test_opt3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)| Optimization 2 | Optimization 3|\n",
    "|:----|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.9314|0.9324| 0.9258|0.9185 | 0.9047|\n",
    "|precision|0.850861|0.844248| 0.790428| 0.779271|0.752134|\n",
    "|recall|0.868752|0.889207| 0.950322|0.932452|0.913152|\n",
    "|f1|0.859583|0.866144| 0.863031| 0.849007|0.824859|\n",
    "\n",
    "\n",
    "#### 5.3.1.4 Optimization 4 - Remove Collinear Features\n",
    "\n",
    "Our fifth optimization step is aimed at eliminating collinear features from the model. In our case, the 'risk_factor' and 'age_group' features exhibit collinearity. Thus, we choose to remove 'risk_factor', which encapsulates certain demographic features, including 'age_group'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('risk_factor'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_opt4 = X_train_opt3.drop(columns=columns_to_drop)\n",
    "y_train_opt4 = y_train_opt3.copy()\n",
    "X_test_opt4 = X_test_opt3.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt4, y_train_opt4, max_depth = 4, X_test = X_test_opt4 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)| Optimization 2 | Optimization 3|Optimization 4|\n",
    "|:----|:----|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.9314|0.9324| 0.9258|0.9185 | 0.9047| 0.9044|\n",
    "|precision|0.850861|0.844248| 0.790428| 0.779271|0.752134|0.750146|\n",
    "|recall|0.868752|0.889207| 0.950322|0.932452|0.913152|0.916369|\n",
    "|f1|0.859583|0.866144| 0.863031| 0.849007|0.824859|0.824968|\n",
    "\n",
    "\n",
    "#### 5.3.1.5 Optimization 5 - Remove Strange Features\n",
    "\n",
    "The next optimization we will attempt is to remove state_fips_code. This feature is an interesting one because it is currently one of the most important factors in predicting patient outcome, but it is being misinterpreted by the model. The model is treating state_fips_codes as continuous numeric values, and testing for it based on relational values when in fact there is no relationship between state number and state. We will try removing it and assessing the impact on the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('state'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_opt5 = X_train_opt4.drop(columns=columns_to_drop)\n",
    "y_train_opt5 = y_train_opt4.copy()\n",
    "X_test_opt5 = X_test_opt4.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt5, y_train_opt5, max_depth = 4, X_test = X_test_opt5 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)| Optimization 2 | Optimization 3|Optimization 4|Optimization 5|\n",
    "|:----|:----|:----|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.9314|0.9324| 0.9258|0.9185 | 0.9047| 0.9044| 0.8996|\n",
    "|precision|0.850861|0.844248| 0.790428| 0.779271|0.752134|0.750146|0.756113\t|\n",
    "|recall|0.868752|0.889207| 0.950322|0.932452|0.913152|0.916369|0.873124|\n",
    "|f1|0.859583|0.866144| 0.863031| 0.849007|0.824859|0.824968|0.810416\t|\n",
    "\n",
    "The inclusion of state_fips_code does not have a significant impact on accuracy or precision, but does significantly reduce recall (~4%). For this reason, we will retain state_fips_code.\n",
    "\n",
    "#### 5.3.1.6 Optimization 6 - Remove More Unnecessary Features\n",
    "\n",
    "Our last optimization will be to attempt to remove symtomatic_tf. It contributes significantly less to the outcome than the other four values remaining in the dataset based on importance factors (about 10% of the next most important factor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('symptomatic'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_opt6 = X_train_opt4.drop(columns=columns_to_drop)\n",
    "y_train_opt6 = y_train_opt4.copy()\n",
    "X_test_opt6 = X_test_opt4.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_random_forest(X_train_opt6, y_train_opt6, max_depth = 4, X_test = X_test_opt6 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Random Forest| Optimization 1  (Oversampled)|Optimization 1  (Undersampled)| Optimization 2 | Optimization 3|Optimization 4|Optimization 5| Optimization 6|\n",
    "|:----|:----|:----|:----|:----|:----|:----|:----|:----|\n",
    "|accuracy|0.9314|0.9324| 0.9258|0.9185 | 0.9047| 0.9044| 0.8996|0.9041|\n",
    "|precision|0.850861|0.844248| 0.790428| 0.779271|0.752134|0.750146|0.756113\t|0.750956|\n",
    "|recall|0.868752|0.889207| 0.950322|0.932452|0.913152|0.916369|0.873124|0.912795|\n",
    "|f1|0.859583|0.866144| 0.863031| 0.849007|0.824859|0.824968|0.810416\t|0.824004|\n",
    "\n",
    "#### 5.5.3.7 Conclusions\n",
    "From the optimizations made in the model, it seems like improvements can be made to the performance of the model, without sacrificing too much of the accuracy of the predictions. We have not been able to successfully mitigate the large disparity in precision and recall for deaths, but the optimizations presented have shrunk the amount of features to only 4 and limited the max depth of the decision tree to 4 whilsty preserving an overall accuracy of 90%+. For completeness, we will also attempt some optimizations on the logistic regression model, which did had better precision and recall values than the random forest model.\n",
    "\n",
    "### 5.3.2 Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_logistic_regression(X_train, y_train, X_test=X_test):\n",
    "\n",
    "    multiple_logisticreg = LogisticRegression().fit(X_train, y_train)\n",
    "    # calculate the prediction and threshold the value. If >= 0.5 its true\n",
    "    multiple_logisticreg_predictions_test = (multiple_logisticreg.predict(X_test)>= 0.5) * 1.0\n",
    "\n",
    "    # Show feature coeffficients\n",
    "    # Create a list of tuples with features and coefficients\n",
    "    feature_coefficients = list(zip(X_train.columns, multiple_logisticreg.coef_.ravel()))\n",
    "    # Sort the list by coefficients in ascending order\n",
    "    sorted_feature_coefficients = sorted(feature_coefficients, key=lambda x: abs(x[1]))\n",
    "    # Display the sorted table using tabulate\n",
    "    display(HTML(tabulate(sorted_feature_coefficients, headers=['Features', 'Coefficients'], tablefmt='html')))\n",
    "\n",
    "    # Using the trained model, on in-sample data (same sample used for training and test), print actual vs predicted values\n",
    "    multiple_logisticreg_predictions_train = (multiple_logisticreg.predict(X_train)>= 0.5) * 1.0\n",
    "    multiple_logisticreg_predictions_test = (multiple_logisticreg.predict(X_test)>= 0.5) * 1.0\n",
    "\n",
    "    actual_vs_predicted_multiplelogisticreg = pd.concat([pd.DataFrame(y_train, columns=['Actual']), pd.DataFrame(multiple_logisticreg_predictions_train, columns=['Predicted'])], axis=1)\n",
    "    actual_vs_predicted_multiplelogisticreg = pd.concat([pd.DataFrame(y_test, columns=['Actual']), pd.DataFrame(multiple_logisticreg_predictions_test, columns=['Predicted'])], axis=1)\n",
    "\n",
    "    # Display classification measures\n",
    "    evaluate_classification('Train', y_train, multiple_logisticreg_predictions_train)\n",
    "    evaluate_classification('Test', y_test, multiple_logisticreg_predictions_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2.1 Optimzation 1 - Remove Unnecessary Features\n",
    "We've already handled oversampling for the logistic regression, so the first optimization we will attempt is to remove unnecessary features. In this step we've defined unnecessary features as features with importance of less than 0.05. The features we will drop from the training and testing data are 'hosp', 'race', 'sex',  'season', 'case_positive_specimen_interval', 'income', 'ethnicity', and 'month_of_year'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority')\n",
    "X_train_log, y_train_log = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('hosp', 'race', 'season', 'sex', 'case_positive_specimen_interval', 'income', 'ethnicity', 'month_of_year'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_log_opt1 = X_train_log.drop(columns=columns_to_drop)\n",
    "y_train_log_opt1 = y_train_log.copy()\n",
    "X_test_log_opt1 = X_test.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_logistic_regression(X_train_log_opt1, y_train_log_opt1, X_test = X_test_log_opt1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Logistic Regression | Optimization 1|\n",
    "|:----|:----|:----|\n",
    "|accuracy|0.9153|0.876|\n",
    "|precision|0.814833|0.0687686|\n",
    "|recall|0.765406|0.908149|\n",
    "|f1|0.789335|0.782689|\n",
    "\n",
    "#### 5.3.2.2 Optimization 2 - Remove Collinear Features\n",
    "We will remove 'risk_factor_tf' in trying to optimize the logistic regression for the same reason that we removed it from the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of column names starting with \"race\", \"sex\", or \"season\"\n",
    "columns_to_drop = [col for col in X_train_opt1.columns if col.startswith(('risk'))]\n",
    "\n",
    "# Drop the specified columns from X_train\n",
    "X_train_log_opt2 = X_train_log_opt1.drop(columns=columns_to_drop)\n",
    "y_train_log_opt2 = y_train_log_opt1.copy()\n",
    "X_test_log_opt2 = X_test_log_opt1.drop(columns=columns_to_drop)\n",
    "\n",
    "train_and_evaluate_logistic_regression(X_train_log_opt2, y_train_log_opt2, X_test = X_test_log_opt2 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference in models is as follows:\n",
    "\n",
    "| |Original Logistic Regression | Optimization 1| Optimization 2|\n",
    "|:----|:----|:----|:----|\n",
    "|accuracy|0.9153|0.876| 0.8772|\n",
    "|precision|0.814833|0.0687686|0.691152|\n",
    "|recall|0.765406|0.908149|0.904575|\n",
    "|f1|0.789335|0.782689|0.783591|\n",
    "\n",
    "In this case it seems like removing this feature actually improves the performance of the predictive model. We will maintain this optimization, removing 'risk_factor_tf' from the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Summary of Findings\n",
    "In our quest to predict COVID-19 outcomes, we attempted several optimizations on our models, specifically on the Random Forest and Logistic Regression models.\n",
    "\n",
    "For the Random Forest model, we addressed class imbalance through oversampling and undersampling, set maximum depth to reduce model complexity, and removed features based on importance, collinearity, and potential misinterpretation by the model. The accuracy of the model oscillated around 90%, with a peak accuracy of 93.24% achieved with oversampling. Precision and recall metrics fluctuated based on the optimization technique, reflecting the balance between over-prediction and under-prediction of patient death. The final model maintained an accuracy above 90% while reducing the feature set to four and limiting the max depth of the decision tree to 4.\n",
    "\n",
    "In comparison, the Logistic Regression model had better initial precision and recall values. We employed similar feature removal techniques as with the Random Forest model. The optimization processes led to a decrease in accuracy from 91.53% to 87.72%, but an increase in recall from 76.54% to 90.46%. The f1 score remained relatively stable across optimizations, indicating a balanced harmonic mean of precision and recall.\n",
    "\n",
    "Both models demonstrated acceptable performance, with each excelling in different aspects. The Random Forest model maintained a higher overall accuracy, while the Logistic Regression model showed better recall. The effectiveness of the models depended on the specific application - whether a higher overall accuracy was preferred, or a model with better recall was needed to prioritize the identification of potential fatalities for preventative measures.\n",
    "\n",
    "In conclusion, our models provided valuable insights into predicting COVID-19 outcomes and potential strategies for refining these models. Further research can build on these findings to create more robust and accurate predictive models, thereby improving our ability to respond effectively in future pandemic situations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "254144bebf7c1be843a3371027c234cfa5e1083251cf625b5fb1113314ea56d5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
